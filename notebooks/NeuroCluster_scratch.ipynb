{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroCluster:\n",
    "<font size= 4> A Python toolbox for nonparametric cluster-based statistical testing of neurophysiological data with respect to continuous predictors \n",
    "\n",
    "First Authors: Alexandra Fink-Skular & Christina Maher  \\\n",
    "Updated: 11/10/2024 by AFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import datetime\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are running NeuroCluster from a local repo (not through a virtual environment package install)\n",
    "user_base_dir   = '/Users/christinamaher/Documents/GitHub/NeuroCluster/'\n",
    "# user_base_dir   = '/Path/To/NeuroCluster/NeuroCluster/'\n",
    "sample_data_dir = f'{user_base_dir}data/synthetic_validation_tfr_data/'\n",
    "\n",
    "# Let's store the date so we can keep track of versions\n",
    "date = datetime.date.today().strftime('%m%d%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/christinamaher/Documents/GitHub/NeuroCluster/NeuroCluster/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{user_base_dir}NeuroCluster/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load custom functions\n",
    "import sys\n",
    "sys.path.append(f'{user_base_dir}NeuroCluster/')\n",
    "import NeuroCluster\n",
    "\n",
    "# load the plotting utils \n",
    "import plotting_utils as plotting_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required packages: ['joblib==1.0.1', 'matplotlib==3.4.0', 'numpy==1.21.0', 'pandas==1.3.0', 'scipy==1.7.0', 'seaborn==0.11.0', 'statsmodels==0.12.2']\n"
     ]
    }
   ],
   "source": [
    "with open(f'{user_base_dir}requirements.txt') as f:\n",
    "    required = f.read().splitlines()\n",
    "\n",
    "print(f'Required packages: {required}')\n",
    "# conda create --name <env> --file <this file>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Format input data (neural and behavioral)\n",
    "\n",
    "The sample data for this notebook includes: \n",
    "- neural data: np.array (n_channels x n_trials x n_freqs x n_times)\n",
    "- behavior data: pd.DataFrame (n_trials x n_variables)\n",
    "\n",
    "These variables are extracted from an `mne.time_frequency.EpochsTFR` which is a spatiotemporal representation of neural data that includes power across trials, frequencies, and timepoints. Note, for this notebook we provide the neural and behavioral data in `np.array` *a priori*. These data can be found in the `sample_data_dir`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some example code, loading an `EpochsTFR` following preprocessing with  [MNE-Python](https://mne.tools/stable/index.html) and creating a neural data file (i.e., `channel_13.npy`) and our behavioral regressors (i.e., `sample_behavior.csv`):\n",
    "\n",
    "`# load EpochsTFR data`\n",
    "\n",
    "\n",
    "`power_epochs = mne.time_frequency.read_tfrs(fname=f'{tfr_dir}{subj_id}_tfr.h5')[0]`\n",
    "\n",
    "\n",
    "\n",
    "`# save data from each channel to a .npy file`\n",
    "\n",
    "\n",
    "`for i in range(len(power_epochs.info['ch_names'])):`\n",
    "\n",
    "        channel = power_epochs.info['ch_names'][i]\n",
    "\n",
    "        data = power_epochs.data[i]\n",
    "\n",
    "        np.save(f'{results_dir}{channel}.npy', data)\n",
    "\n",
    "\n",
    "\n",
    "`# save metadata as a numpy array to a .csv file`\n",
    "\n",
    "\n",
    "`metadata = pd.DataFrame(power_epochs.metadata)`\n",
    "\n",
    "\n",
    "`metadata.to_csv(f'{results_dir}sample_behavior.csv', index=False)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, let's load our template neural data into a dictionary and load the DataFrame of our behavioral predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/christinamaher/Documents/GitHub/NeuroCluster/data/synthetic_validation_tfr_data/synthetic_validation_tfr.fif ...\n",
      "Adding metadata with 1 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5p/bctklsps5kgb5fk5s1nygs3m0000gn/T/ipykernel_20192/2506304949.py:7: RuntimeWarning: This filename (/Users/christinamaher/Documents/GitHub/NeuroCluster/data/synthetic_validation_tfr_data/synthetic_validation_tfr.fif) does not conform to MNE naming conventions. All tfr files should end with -tfr.h5 or _tfr.h5\n",
      "  tfr = mne.time_frequency.read_tfrs(sample_ieeg_file)\n"
     ]
    }
   ],
   "source": [
    "# read fif file from data directory\n",
    "sample_ieeg_file = f'{sample_data_dir}synthetic_validation_tfr.fif'\n",
    "# use mne to load \n",
    "import mne\n",
    "\n",
    "# Load the TFR data\n",
    "tfr = mne.time_frequency.read_tfrs(sample_ieeg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expected_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.959596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.969697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.979798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.989899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    expected_value\n",
       "0         0.000000\n",
       "1         0.010101\n",
       "2         0.020202\n",
       "3         0.030303\n",
       "4         0.040404\n",
       "..             ...\n",
       "95        0.959596\n",
       "96        0.969697\n",
       "97        0.979798\n",
       "98        0.989899\n",
       "99        1.000000\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the behavioral metadata there should be an EV column\n",
    "sample_behav = tfr[0].metadata\n",
    "sample_behav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['synthetic_channel']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print channel names in tfr data\n",
    "tfr[0].info['ch_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the behavioral variables we plan to include as independent variables in our linear regression (`multi_reg_vars`) and the regressor of interest (`target_var`) we will permute to determine whether significant clusters encoding this behavioral variable exist in our time x frequency data. All continuous predictors should be normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>expected_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.030303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.040404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   expected_value\n",
       "0        0.000000\n",
       "1        0.010101\n",
       "2        0.020202\n",
       "3        0.030303\n",
       "4        0.040404"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set main predictor of interest for permutations *target_var must be a continuous numeric variable*\n",
    "target_var = 'expected_value'\n",
    "\n",
    "# define subset of predictor variables from sample_behav to include in regression (should include target_var)\n",
    "multi_reg_vars = ['expected_value']\n",
    "\n",
    "# subset input dataframe to include only multi_reg_vars\n",
    "predictor_data = sample_behav.copy()[multi_reg_vars]\n",
    "\n",
    "# let's print the first few rows of the predictor data to make sure it looks right\n",
    "predictor_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr_data = tfr[0].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 30, 500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfr_data.shape\n",
    "\n",
    "# get rid of dimension = 1 in tfr_data\n",
    "tfr_data = tfr_data.squeeze()\n",
    "tfr_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Perform within-electrode cluster test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a variable called `tfr_data` which is a `np.array` (dimensions should correspond with number of trials x number of frequencies x number of timepoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trials for synthetic_channel: 100\n",
      "Number of frequencies for synthetic_channel: 30\n",
      "Number of timepoints for synthetic_channel: 500\n"
     ]
    }
   ],
   "source": [
    "# subset demo channel data from sample_ieeg_dict and store as tfr_data variable: np.array of (num epochs x num frequencies x num times)\n",
    "demo_channel = 'synthetic_channel'\n",
    "#tfr_data     = sample_ieeg_dict[demo_channel]\n",
    "\n",
    "\n",
    "# check tfr_data dimensions - must be num trials, num frequencies, num timepoints\n",
    "tfr_data.shape \n",
    "print(f'Number of trials for {demo_channel}: {tfr_data.shape[0]}')\n",
    "print(f'Number of frequencies for {demo_channel}: {tfr_data.shape[1]}')\n",
    "print(f'Number of timepoints for {demo_channel}: {tfr_data.shape[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create an instance of `TFR_Cluster_Test`. This will be used to run the cluster test. It requires the tfr_data, predictor_data, target_var, and demo_channel as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(NeuroCluster.TFR_Cluster_Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_test  = NeuroCluster.TFR_Cluster_Test(tfr_data,predictor_data,target_var,demo_channel)\n",
    "cluster_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate t-critical for a two-sided hypothesis test, we compute a T-distribution with N-K-1 degrees of freedom (N=number of samples, K = number of predictors in regression model) and find the t-values where the area of the t-distribution = 0.025 and 0.975 (1-alpha/ntails,alpha=0.05,ntails=2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_utils.plot_tcritical(cluster_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are reading to run our linear regression based on the dependent neural variable (`tfr_data`) and independent behavioral variables (`predictor_data`, `target_var`) we passed as inputs to our `TFR_Cluster_Test` object. This will return pixel-level **β coefficients** and corresponding **t-statistics** for our TFR data in one electrode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas, tstats = cluster_test.tfr_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the **β coefficients** to give us an idea of the neural encoding pattern for our continuous predictor of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuroCluster.plot_beta_coef(betas,cluster_test,freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the **t-statistics** that correspond with the **β coefficient** for each time-frequency point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuroCluster.plot_tstats(tstats,cluster_test,freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's highlight **clusters** (defined as consecutive time x frequency points) with significant t-statistics. We can do this separately for both positive and negative clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuroCluster.plot_clusters(tstats,cluster_test,freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Calculate True Cluster Statistic(s) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will identify the largest cluster (either/both positive and negative) and save the **cluster statistic** which will be our test statistic against our non-parametric null distribution. `max_tfr_cluster()` returns a dictionary containing the **cluster statistic**:`cluster_stat` and its associated **freq_idx**: `freq_idx` and **time_idx**:`time_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Find largest cluster(s) and return the max cluster statistic(s) and cluster's  frequencies x times indices\n",
    "max_cluster_data  = cluster_test.max_tfr_cluster(tstats,max_cluster_output='all')\n",
    "print(f'Max positive cluster dictionary: {max_cluster_data[0]}')\n",
    "print(f'Max negative cluster dictionary: {max_cluster_data[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our largest cluster and its associated **cluster statistic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuroCluster.plot_max_clusters(cluster_test,tstats,freqs)\n",
    "# TFR-Level Test Statistic: Largest Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Compute cluster p-value(s) from null distribution of cluster statistics. \n",
    "To generate the null distribution, perform non-parametric cluster-based permutation testing by randomly permuting predictor of interest (target_var). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have computed the true cluster statistics from our neural (`tfr_data`) and behavioral data (`predictor_data`, `target_var`). Next, we will permute our input data and re-run the cluster identification procedure on each permuted dataset. This will allow us to generate a null distribution of cluster statistics, which we can use to evaluate the statistical significance of the cluster statistics observed in our true data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute_null_cluster_stats()` takes `num_permutations` as an input, which specifies the desired number of permutations. The function will permute the regressor of interest according to this number. It returns a list of null cluster statistics, with the length of the list depending on the tails of the test. Here we generated 100 null cluster statistics, `num_permutations=100`, but we recommend running at least 200 permutations (500 to 1000 is best practice). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cluster_distribution = cluster_test.compute_null_cluster_stats(num_permutations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cluster_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the ***p*-value** associated with our true cluster statistics based on the null distributions we create using `cluster_significance_test()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pvalue = cluster_test.cluster_significance_test(max_cluster_data,null_cluster_distribution) \n",
    "print(f'Positive cluster p-value: {cluster_pvalue[0]}')\n",
    "print(f'Negative cluster p-value: {cluster_pvalue[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a plot showing the **null distribution(s)** we generated, with our true cluster statistic overlaid on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuroCluster.plot_null_distribution(null_cluster_distribution, max_cluster_data,cluster_pvalue,dpi=125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all plots\n",
    "tstat_threshold = cluster_test.threshold_tfr_tstat(tstats)\n",
    "tcrit_plot,beta_plot,tstat_plot,cluster_plot,max_cluster_plot,null_distribution_plot = NeuroCluster.plot_neurocluster_results(betas,cluster_test,\n",
    "                                                                                                                    max_cluster_data, null_cluster_distribution, tstats, tstat_threshold,cluster_pvalue,freqs)\n",
    "\n",
    "# Define the directory where you want to save the plots\n",
    "output_directory = f'{results_dir}/{demo_channel}_{target_var}'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "NeuroCluster.create_directory(output_directory)\n",
    "\n",
    "# Save plots to the output directory\n",
    "NeuroCluster.save_plot_to_pdf(tcrit_plot, output_directory, f'{cluster_test.alternative}_tcrit_plot.pdf')\n",
    "NeuroCluster.save_plot_to_pdf(beta_plot, output_directory, f'{cluster_test.alternative}_beta_plot.pdf')\n",
    "NeuroCluster.save_plot_to_pdf(tstat_plot, output_directory, f'{cluster_test.alternative}_tstat_plot.pdf')\n",
    "NeuroCluster.save_plot_to_pdf(cluster_plot, output_directory, f'{cluster_test.alternative}_cluster_plot.pdf')\n",
    "NeuroCluster.save_plot_to_pdf(max_cluster_plot, output_directory, f'{cluster_test.alternative}_max_cluster_plot.pdf')\n",
    "NeuroCluster.save_plot_to_pdf(null_distribution_plot, output_directory, f'{cluster_test.alternative}_null_distribution_plot.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Sided Hypothesis Test Example\n",
    "Rather than testing whether a tfr cluster significantly encodes our `target_var` in general, we can evaluate the directionality of `target_var` encoding in our cluster. Specifically, we can test whether neuronal activity in a cluster increases (or decreases) with increasing (or decreasing) values of the `target_var`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_test  = NeuroCluster.TFR_Cluster_Test(tfr_data,predictor_data,target_var,demo_channel,alternative='greater')\n",
    "cluster_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate t-critical for a one-sided hypothesis test, we compute a T-distribution with N-K-1 degrees of freedom (N=number of samples, K = number of predictors in regression model) and find the t-value where the area of the t-distribution = 0.95 (1-alpha,alpha=0.05). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuroCluster.plotting_utils.plot_tcritical(cluster_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are reading to run our linear regression based on the dependent neural variable (`tfr_data`) and independent behavioral variables (`predictor_data`, `target_var`) we passed as inputs to our `TFR_Cluster_Test` object. This will return pixel-level **β coefficients** and corresponding **t-statistics** for our TFR data in one electrode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betas, tstats = cluster_test.tfr_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the **β coefficients** to give us an idea of the neural encoding pattern for our continuous predictor of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuroCluster.plot_beta_coef(betas,cluster_test,freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the **t-statistics** that correspond with the **β coefficient** for each time-frequency point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuroCluster.plot_tstats(tstats,cluster_test,freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's highlight **clusters** (defined as consecutive time x frequency points) with significant t-statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuroCluster.plot_clusters(tstats,cluster_test,freqs,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Calculate True Cluster Statistic(s) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will identify the largest cluster (either/both positive and negative) and save the **cluster statistic** which will be our test statistic against our non-parametric null distribution. `max_tfr_cluster()` returns a dictionary containing the **cluster statistic**:`cluster_stat` and its associated **freq_idx**: `freq_idx` and **time_idx**:`time_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Find largest cluster(s) and return the max cluster statistic(s) and cluster's  frequencies x times indices\n",
    "max_cluster_data  = cluster_test.max_tfr_cluster(tstats,max_cluster_output='all')\n",
    "print(f'Max positive cluster dictionary: {max_cluster_data[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our **positive** largest cluster and its associated **cluster statistic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuroCluster.plot_max_clusters(cluster_test,tstats,freqs,figsize=(6,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Compute cluster p-value(s) from null distribution of cluster statistics. \n",
    "To generate the null distribution, perform non-parametric cluster-based permutation testing by randomly permuting predictor of interest (target_var). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have computed the true cluster statistics from our neural (`tfr_data`) and behavioral data (`predictor_data`, `target_var`). Next, we will permute our input data and re-run the cluster identification procedure on each permuted dataset. This will allow us to generate a null distribution of cluster statistics, which we can use to evaluate the statistical significance of the cluster statistics observed in our true data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`compute_null_cluster_stats()` takes `num_permutations` as an input, which specifies the desired number of permutations. The function will permute the regressor of interest according to this number. It returns a list of null cluster statistics, with the length of the list depending on the tails of the test. Here we generated 100 null cluster statistics, `num_permutations=100`, but we recommend running at least 200 permutations (500 to 1000 is best practice). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cluster_distribution = cluster_test.compute_null_cluster_stats(num_permutations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the ***p*-value** associated with our true cluster statistics based on the null distributions we create using `cluster_significance_test()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pvalue = cluster_test.cluster_significance_test(max_cluster_data,null_cluster_distribution) \n",
    "print(f'Positive cluster p-value: {cluster_pvalue[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a plot showing the **null distribution(s)** we generated, with our true cluster statistic overlaid on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NeuroCluster.plot_null_distribution(null_cluster_distribution, max_cluster_data,cluster_pvalue,figsize=(6,5),dpi=125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to replicate findings using MNE's built-in t-test functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if we get the same result as Neurocluster (a significant positive cluster meaning as error increases, encoding increases), using mne's built-in two sample t-test cluster function\n",
    "import mne\n",
    "\n",
    "# Let's take a median split for the error variable and assign it to high and low error variables (we need to discretize the error variable for the mne function)\n",
    "error = sample_behav['error']\n",
    "median_error = np.median(error)\n",
    "low_error = tfr_data[error < median_error, :, :]    \n",
    "high_error = tfr_data[error >= median_error, :, :]\n",
    "\n",
    "# Let's run the mne two sample t-test cluster function\n",
    "t_obs, clusters, cluster_pv, H0 = mne.stats.permutation_cluster_test([low_error, high_error], n_permutations=1000, tail=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_c, c in enumerate(clusters):\n",
    "    c = c[0]\n",
    "    if cluster_pv[i_c] <= 0.05:\n",
    "        print(f\"Cluster {i_c} p-value: {cluster_pv[i_c]}\")\n",
    "    else:\n",
    "        print(\"No significant clusters found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's run the entire pipeline at once and save plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1688 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 15000 out of 15000 | elapsed:    4.7s finished\n"
     ]
    }
   ],
   "source": [
    "### NeuroCluster single electrode workflow: \n",
    "\n",
    "# Step 1: Create TFR_Cluster_Test Object\n",
    "cluster_test  = NeuroCluster.TFR_Cluster_Test(tfr_data,predictor_data,target_var,demo_channel,alternative='two-sided')\n",
    "\n",
    "# Step 2: Run TFR regression to extract beta coefficients for predictor of interest (permute_var) & tstats for each pixel in TFR. Determine which t-statistics are significant based on the critical t-value and save a thresholded t-statistic matrix.\n",
    "betas, tstats = cluster_test.tfr_regression()\n",
    "tstat_threshold = cluster_test.threshold_tfr_tstat(tstats)\n",
    "\n",
    "# Step 3: Find largest cluster(s) and return the max cluster statistic(s) and cluster's  frequencies x times indices\n",
    "max_cluster_data  = cluster_test.max_tfr_cluster(tstats,max_cluster_output='all')\n",
    "\n",
    "# Step 4: Create null distribution of maximum cluster statistics from permuted data\n",
    "null_cluster_distribution = cluster_test.compute_null_cluster_stats(num_permutations=100)\n",
    "\n",
    "# Step 5: Use null cluster statistic distribution from permutations to compute non-parametric p value \n",
    "cluster_pvalue = cluster_test.cluster_significance_test(max_cluster_data,null_cluster_distribution) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotting_utils as plotting_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot all the steps together and save the figures to a dictory that corresponds to the channel and predictor of interest. \n",
    "freqs = np.logspace(*np.log10([2, 200]), num=30)\n",
    "tcrit_plot,beta_plot,tstat_plot,cluster_plot,max_cluster_plot,null_distribution_plot = plotting_utils.plot_neurocluster_results(betas,cluster_test, max_cluster_data, null_cluster_distribution, tstats, tstat_threshold,cluster_pvalue, freqs)\n",
    "\n",
    "# Define the directory where you want to save the plots\n",
    "output_directory = '/Users/christinamaher/Documents/GitHub/NeuroCluster/data/synthetic_validation_tfr_data/'\n",
    "\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_plot_to_pdf(fig, directory, filename):\n",
    "    \"\"\"Save a plot to the specified directory with the given filename.\"\"\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    fig.savefig(filepath, dpi=300,bbox_inches='tight')\n",
    "    plt.close(fig)  # Close the figure to avoid display and memory issues\n",
    "\n",
    "# Save plots to the output directory\n",
    "save_plot_to_pdf(beta_plot, output_directory, 'beta_plot.png')\n",
    "save_plot_to_pdf(tstat_plot, output_directory, 'tstat_plot.png')\n",
    "save_plot_to_pdf(cluster_plot, output_directory, 'cluster_plot.png')\n",
    "save_plot_to_pdf(max_cluster_plot, output_directory, 'max_cluster_plot.png')\n",
    "save_plot_to_pdf(null_distribution_plot, output_directory, 'null_distribution_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting at 0 get every 6th freq\n",
    "freqs[::6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_freqs = np.linspace(freqs[0], freqs[-1], num_yticks) \n",
    "selected_freqs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
