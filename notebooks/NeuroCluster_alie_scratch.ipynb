{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroCluster:\n",
    "<font size= 4> Non-parametric cluster-based permutation testing to identify neurophysiological encoding of continuous variables with time-frequency resolution\n",
    "\n",
    "Authors: Christina Maher & Alexandra Fink-Skular \\\n",
    "Updated: 06/24/2024 by AFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from glob import glob\n",
    "from scipy.stats import zscore, t, linregress, ttest_ind, ttest_rel, ttest_1samp \n",
    "import os \n",
    "import re\n",
    "import h5io\n",
    "import pickle \n",
    "import time \n",
    "import datetime \n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm \n",
    "from scipy.ndimage import label \n",
    "import statsmodels.formula.api as smf\n",
    "import tqdm\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# keep this so we can use our respective paths for testing\n",
    "current_user = 'alie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07162024\n"
     ]
    }
   ],
   "source": [
    "date = datetime.date.today().strftime('%m%d%Y')\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_user == 'christina':\n",
    "    base_dir = '/Users/christinamaher/Documents/GitHub/NeuroCluster/scripts/'\n",
    "    data_dir = '/Users/christinamaher/Documents/GitHub/NeuroCluster/'\n",
    "    tfr_dir  = f'{data_dir}tfr/'\n",
    "    anat_dir = f'{data_dir}anat/'\n",
    "elif current_user == 'alie':\n",
    "    # base_dir = '/hpc/users/finka03/NeuroCluster/NeuroCluster/'\n",
    "    # swb_dir  = '/sc/arion/projects/guLab/Alie/SWB/'\n",
    "    # tfr_dir  = f'{swb_dir}ephys_analysis/data/'\n",
    "    # beh_dir  = f'{swb_dir}swb_behav_models/data/behavior_preprocessed/'\n",
    "    # anat_dir = f'{swb_dir}ephys_analysis/recon_labels/'\n",
    "    # save_dir = f'{base_dir}/data/'\n",
    "    \n",
    "    base_dir = '/Users/alexandrafink/Documents/GraduateSchool/SaezLab/NeuroCluster/NeuroCluster/NeuroCluster/scripts/'\n",
    "    data_dir = '/Users/alexandrafink/Documents/GraduateSchool/SaezLab/SWB/'\n",
    "    tfr_dir  = f'{data_dir}ephys_analysis/data/'\n",
    "    beh_dir  = f'{data_dir}behavior_analysis/behavior_preprocessed/'\n",
    "    anat_dir = f'{data_dir}anat_recons/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load functions \n",
    "import sys\n",
    "sys.path.append(f'{base_dir}')\n",
    "# sys.path.append(f'{base_dir}scripts/')\n",
    "\n",
    "from tfr_cluster_test import *\n",
    "from helper_utils import *\n",
    "# from plotting_utils import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Format Input Data (Currently within-subject)\n",
    "- neural input: np.array (n_channels x n_epochs x n_freqs x n_times)\n",
    "- regressor data: np.array (numpy array: n_epochs x n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/alexandrafink/Documents/GraduateSchool/SaezLab/SWB/ephys_analysis/data/MS002/MS002_CpeOnset-tfr.h5 ...\n",
      "Adding metadata with 19 columns\n"
     ]
    }
   ],
   "source": [
    "# load epoched data for single subj\n",
    "if current_user == 'alie':\n",
    "    permute_var = 'decisionCPE'\n",
    "    subj_id     = 'MS002'   \n",
    "    power_epochs = mne.time_frequency.read_tfrs(fname=f'{tfr_dir}{subj_id}/{subj_id}_CpeOnset-tfr.h5')[0]\n",
    "elif current_user == 'christina':\n",
    "    permute_var = 'ev_zscore'\n",
    "    subj_id     = 'MS009'   \n",
    "    power_epochs = mne.time_frequency.read_tfrs(fname=f'{tfr_dir}/{subj_id}_tfr.h5')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ROI for single ROI anaylsis \n",
    "if current_user == 'alie':\n",
    "#     roi = 'ains'\n",
    "    # set all variables included mutliple regression \n",
    "    multi_reg_vars = ['GambleChoice','TrialEV','decisionCPE']\n",
    "    # set main variable of interest for permutations \n",
    "    permute_var = 'decisionCPE'\n",
    "    # load subj behavior data \n",
    "#     beh_df = pd.read_csv(f'{beh_dir}{subj_id}_task_data')\n",
    "    beh_df = power_epochs.metadata.copy()\n",
    "    # beh_df['subj_id'] = subj_id\n",
    "    # add TrialEV to df\n",
    "    beh_df['TrialEV'] = beh_df.GambleEV - beh_df.SafeBet\n",
    "    # clean subj dataframe from fail trials/nan values in vars of interest     \n",
    "    # beh_df = beh_df[(beh_df.GambleChoice=='gamble')|(beh_df.GambleChoice=='safe')]\n",
    "#     beh_df = beh_df[(beh_df.Outcome=='good')|(beh_df.Outcome=='bad')]\n",
    "    \n",
    "    # zscore continuous variables \n",
    "    beh_df[multi_reg_vars[1:]] = pd.DataFrame({f'{var}':zscore(beh_df[var])  for var in multi_reg_vars[1:]})\n",
    "    # format final beh_df\n",
    "    beh_df = beh_df[multi_reg_vars].reset_index(drop=True) \n",
    "    # convert choice to categorical variable\n",
    "    beh_df['GambleChoice'] = beh_df['GambleChoice'].astype('category')\n",
    "\n",
    "elif current_user == 'christina':\n",
    "    beh_df = prepare_regressor_df(power_epochs)\n",
    "    ## new function for getting elecs in ROI\n",
    "    roi = ['lpfc','ofc']\n",
    "    roi_subj_elecs = prepare_anat_dic(roi, f'{anat_dir}master_labels.csv')\n",
    "    roi_subj_elecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### class TFR_Cluster_Test dev + debugging\n",
    "\n",
    "if current_user == 'alie':\n",
    "\n",
    "    # subset single electrode tfr data + behav data\n",
    "    dev_ch_idx = power_epochs.ch_names.index('laims2-laims3')\n",
    "    ch_name    = 'laims2-laims3'\n",
    "    tfr_data   = np.squeeze(power_epochs._data[:,dev_ch_idx,:,:].copy())\n",
    "    predictor_data = beh_df.copy()\n",
    "    \n",
    "    # predictor_data = predictor_data.drop(columns='subj_id')\n",
    "\n",
    "elif current_user == 'christina':\n",
    "    \n",
    "        # subset single electrode tfr data + behav data\n",
    "        # predictor_data = predictor_data.drop(columns=['condition','chosen_shape_current_trial','chosen_color_current_trial','chosen_shape_previous_trial','chosen_color_previous_trial','ev'])\n",
    "        tfr_data = np.squeeze(power_epochs._data[:,0,:,:].copy())\n",
    "        ch_name = power_epochs.info['ch_names'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GambleChoice', 'TrialEV', 'decisionCPE'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Find Real Clusters\n",
    "- Use TFRClusterTest class code to run multivariate regression\n",
    "- Allows for multiple regression implementation and pixel paralellization, so with more speed improvements will ultimately be worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TFR_Cluster_Test at 0x7fd4b1114730>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_test  = TFR_Cluster_Test(tfr_data,predictor_data,permute_var,ch_name,alternative='two-sided')\n",
    "cluster_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1616 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 5648 tasks      | elapsed:    9.1s\n",
      "[Parallel(n_jobs=-1)]: Done 10832 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 17168 tasks      | elapsed:   19.4s\n",
      "[Parallel(n_jobs=-1)]: Done 24656 tasks      | elapsed:   25.8s\n",
      "[Parallel(n_jobs=-1)]: Done 33296 tasks      | elapsed:   33.5s\n",
      "[Parallel(n_jobs=-1)]: Done 43088 tasks      | elapsed:   42.5s\n",
      "[Parallel(n_jobs=-1)]: Done 45030 out of 45030 | elapsed:   45.5s finished\n"
     ]
    }
   ],
   "source": [
    "betas, tstats = cluster_test.tfr_regression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.10794783, -0.2703095 , -0.41794274, -0.44248438, -0.37458303,\n",
       "       -0.2818333 , -0.60573255, -0.19793927, -0.48292876, -0.47157297,\n",
       "       -0.36890248,  0.07186986,  0.95611905,  1.93668282,  1.30705958,\n",
       "        0.71583473,  0.30654092, -0.33094094,  0.45723406,  1.49685098,\n",
       "        1.34073302,  0.49428584, -0.35482192,  0.04031469,  0.6298716 ,\n",
       "        1.7727532 , -0.51709544, -0.26340763, -0.72451961, -0.12702793])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tstats[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GambleChoice</th>\n",
       "      <th>TrialEV</th>\n",
       "      <th>decisionCPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>safe</td>\n",
       "      <td>-0.871214</td>\n",
       "      <td>-0.800927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gamble</td>\n",
       "      <td>-0.551785</td>\n",
       "      <td>-0.899141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gamble</td>\n",
       "      <td>1.630983</td>\n",
       "      <td>1.470249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>safe</td>\n",
       "      <td>0.779171</td>\n",
       "      <td>0.880971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gamble</td>\n",
       "      <td>0.495234</td>\n",
       "      <td>0.807311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>gamble</td>\n",
       "      <td>-0.356578</td>\n",
       "      <td>0.512672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>safe</td>\n",
       "      <td>-0.179117</td>\n",
       "      <td>-0.297586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>gamble</td>\n",
       "      <td>-0.640515</td>\n",
       "      <td>-1.206056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>gamble</td>\n",
       "      <td>0.495234</td>\n",
       "      <td>-0.469458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>safe</td>\n",
       "      <td>-1.492327</td>\n",
       "      <td>1.249270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    GambleChoice   TrialEV  decisionCPE\n",
       "0           safe -0.871214    -0.800927\n",
       "1         gamble -0.551785    -0.899141\n",
       "2         gamble  1.630983     1.470249\n",
       "3           safe  0.779171     0.880971\n",
       "4         gamble  0.495234     0.807311\n",
       "..           ...       ...          ...\n",
       "145       gamble -0.356578     0.512672\n",
       "146         safe -0.179117    -0.297586\n",
       "147       gamble -0.640515    -1.206056\n",
       "148       gamble  0.495234    -0.469458\n",
       "149         safe -1.492327     1.249270\n",
       "\n",
       "[150 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.04306955e-01, -3.40740341e-01,  6.16572718e-01,  4.04204080e-01,\n",
       "       -8.30105119e-01, -7.44620041e-01,  8.44368701e-01, -7.93847949e-01,\n",
       "        1.57998057e-01, -5.71022920e-01, -4.70227422e-01,  6.47199078e-01,\n",
       "       -5.11724316e-02, -7.91471738e-01,  1.74719691e-01,  2.01581159e+00,\n",
       "       -2.90361140e-01,  6.97568770e-01, -8.88295574e-01, -3.78137855e-01,\n",
       "       -3.51952254e-01, -3.41832627e-01, -9.72941641e-01,  1.96794010e-04,\n",
       "       -7.33041204e-01, -8.40549586e-01, -8.28619537e-01,  9.77712994e-01,\n",
       "       -5.22935601e-01,  6.21032670e-01,  3.42171079e-01,  1.33514279e+00,\n",
       "       -8.42726760e-01,  1.91532118e-01, -3.45197140e-01,  8.48316400e-02,\n",
       "       -6.37739153e-01, -8.48490267e-01,  2.98054646e-02, -6.35622745e-01,\n",
       "       -7.18227805e-01, -5.96076465e-01, -8.85158208e-02,  2.14791389e-01,\n",
       "       -8.34611845e-01, -7.28894457e-01,  1.45311944e+00, -5.16088622e-01,\n",
       "       -7.63661895e-01,  1.44519950e+00,  2.83421658e+00,  2.05710105e+00,\n",
       "        1.21125195e+00, -8.18442284e-01,  4.70699519e-01, -3.04398937e-01,\n",
       "        1.30294081e-01,  2.07967609e+00, -6.63190369e-01, -7.23989779e-01,\n",
       "       -8.16581349e-01, -7.90115547e-01,  1.68942048e+00, -8.79772626e-01,\n",
       "        2.90275272e-01, -7.30163435e-01,  1.55382399e+00,  4.07555606e-02,\n",
       "        2.19571556e-01,  2.07604393e+00,  6.88356132e-02, -5.90299217e-01,\n",
       "        7.50420416e-01, -9.61695238e-01,  2.77449909e-01, -5.94929594e-01,\n",
       "       -3.96294504e-01, -4.16710438e-01,  1.37797361e+00, -7.45320114e-01,\n",
       "       -9.10461013e-01, -4.61113450e-02,  1.35776781e-01,  8.25959891e-01,\n",
       "        2.31029768e+00, -7.21988603e-01,  4.04707354e-01, -5.23046151e-01,\n",
       "        1.53176055e+00, -6.28220646e-01, -9.18737949e-01, -5.45047331e-01,\n",
       "       -5.91286742e-01, -6.31936568e-01, -9.23718520e-02, -5.11561199e-01,\n",
       "        1.02862633e+00,  7.70237949e-01,  2.41440612e+00, -6.92385706e-01,\n",
       "       -9.34208327e-01, -4.19276088e-01, -4.23288009e-01, -6.66161499e-01,\n",
       "       -4.58879508e-01,  4.98295797e-02, -6.80270932e-01, -7.31526994e-01,\n",
       "        1.02934995e+00, -7.07634407e-01,  1.31983415e-01, -9.47363118e-01,\n",
       "       -8.55826086e-01, -6.68228184e-01,  1.28883185e+00,  3.64918468e-02,\n",
       "       -8.22396258e-01, -6.88034825e-01,  6.15073095e-01, -7.01495363e-01,\n",
       "       -7.73570473e-01,  1.65462077e-01, -5.28652844e-01, -7.36570781e-01,\n",
       "        1.65756910e+00, -8.90626677e-01, -9.07053585e-01, -3.54239057e-01,\n",
       "       -5.78192225e-01,  7.19208236e-01,  2.79051087e+00,  3.07027104e+00,\n",
       "       -6.01673288e-01,  7.40421141e-01,  6.78742504e-01, -8.27096672e-01,\n",
       "       -4.79962713e-01, -1.56538591e-02, -1.71105643e-01, -6.24778321e-01,\n",
       "        1.22033188e+00, -8.31881581e-01,  1.21093788e-01,  4.20242204e-01,\n",
       "       -5.19409074e-01, -7.10649505e-01, -7.95110211e-01, -7.78948359e-01,\n",
       "       -1.66994985e-01,  2.81631739e-01])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfr_data[:,-1,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cluster_data  = cluster_test.max_tfr_cluster(tstats,output='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    results = Parallel(n_jobs=n_jobs, verbose=5)(delayed(min_fn)(fit_fn, param_values, (subj_df), param_bounds) for param_values in param_combo_guesses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NeuroCluster single electrode workflow: \n",
    "\n",
    "# Step 1: Create TFR_Cluster_Test Object\n",
    "cluster_test  = TFR_Cluster_Test(tfr_data,predictor_data,permute_var,ch_name,alternative='two-sided')\n",
    "\n",
    "# Step 2: Run TFR regression to extract beta coefficients for predictor of interest (permute_var) & tstats for each pixel in TFR\n",
    "betas, tstats = cluster_test.tfr_regression()\n",
    "\n",
    "# Step 3: Find largest cluster(s) and return the max cluster statistic(s) and cluster's  frequencies x times indices\n",
    "max_cluster_data  = cluster_test.max_tfr_cluster(tstats,output='all')\n",
    "\n",
    "# Step 4: Create null distribution of maximum cluster statistics from permuted data\n",
    "null_cluster_distribution = cluster_test.compute_null_cluster_stats(max_cluster_data,num_permutations=10)\n",
    "\n",
    "# Step 5: Use null cluster statistic distribution from permutations to compute non-parametric p value \n",
    "cluster_pvalue = cluster_test.cluster_significance_test(null_cluster_distribution,alpha=0.05) #compute_cluster_pvalue cluster_significance_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFR_Cluster_Test(object):\n",
    "    \"\"\" \n",
    "    Single-electrode neurophysiology object class to identify time-frequency resolved neural activity correlates of complex behavioral variables using non-parametric \n",
    "    cluster-based permutation testing.   \n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    tfr_data       : (np.array) Single electrode tfr data matrix. Array of floats (n_epochs,n_freqs,n_times). \n",
    "    tfr_dims       : (tuple) Frequency and time dimensions of tfr_data. Tuple of integers (n_freq,n_times). \n",
    "    ch_name        : (str) Unique electrode identification label. String of characters.\n",
    "    predictor_data : (pd.DataFrame) Regressors from task behavior with continuous, discreet, or categorical data. DataFrame of (rows=n_epochs,columns=n_regressors). \n",
    "    permute_var    : (str) Column label for primary regressor of interest.\n",
    "      \n",
    "    Methods\n",
    "    ----------\n",
    "    **To-do: fill in methods info\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tfr_data, predictor_data, permute_var, ch_name, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - tfr_data       : (np.array) Single electrode tfr data matrix. Array of floats (n_epochs,n_freqs,n_times). \n",
    "        - predictor_data : (pd.DataFrame) Task-based regressor data with dtypes continuous/discreet(int64/float) or categorical(pd.Categorical). DataFrame of (n_epochs,n_regressors).\n",
    "        - permute_var    : (str) Column label for primary regressor of interest. Array of 1d integers or floats (n_epochs,).\n",
    "        - ch_name        : (str) Unique electrode identification label. String of characters.  \n",
    "        - **kwargs       : (optional) alternative, alpha, cluster_shape\n",
    "        \"\"\"\n",
    "\n",
    "        self.tfr_data       = tfr_data  # single electrode tfr data\n",
    "        self.predictor_data = predictor_data # single subject behav data\n",
    "        self.tfr_dims       = self.tfr_data.shape[1:] # time-frequency dims of electrode data (n_freqs x n_times)\n",
    "        self.permute_var    = permute_var # variable to permute in regression model \n",
    "        self.ch_name        = ch_name # channel name for single electrode tfr data\n",
    "\n",
    "    def tfr_regression(self):\n",
    "        \"\"\"\n",
    "        Performs univariate or multivariate OLS regression across tfr matrix for all pixel-level time-frequency power data and task-based predictor variables. Regressions are parallelized across pixels.\n",
    "\n",
    "        Returns:\n",
    "        - tfr_betas  : (np.array) Matrix of beta coefficients for predictor of interest for each pixel regression. Array of (n_freqs,n_times). \n",
    "        - tfr_tstats : (np.array) Matrix of t-statistics from coefficient estimates for predictor of interest for each pixel regression. Array of (n_freqs,n_times). \n",
    "        \"\"\"\n",
    "        \n",
    "        # Prepare arguments for parallelization`using tfr matrix indices converted to list of tuples (freq x power)\n",
    "        pixel_args = [self.make_pixel_df(self.tfr_data[:,freq_idx,time_idx]) for freq_idx,time_idx in self.expand_tfr_indices()]\n",
    "        \n",
    "        # run pixel permutations in parallel \n",
    "        expanded_results = Parallel(n_jobs=-1, verbose=5)(\n",
    "                        delayed(self.pixel_regression)(args)\n",
    "                            for args in pixel_args)      \n",
    "\n",
    "        # preallocate np arrays for betas + tstats\n",
    "        tfr_betas  = np.zeros((self.tfr_dims))\n",
    "        tfr_tstats = np.zeros((self.tfr_dims))\n",
    "\n",
    "        # expanded_results is a list of tuples (beta,tstat) for every pixel \n",
    "        for count,(freq_idx,time_idx) in enumerate(self.expand_tfr_indices()):\n",
    "            tfr_betas[freq_idx,time_idx]  = expanded_results[count][0]\n",
    "            tfr_tstats[freq_idx,time_idx] = expanded_results[count][1]\n",
    "        \n",
    "        return tfr_betas, tfr_tstats\n",
    "\n",
    "    def pixel_regression(self,pixel_df):\n",
    "        \"\"\"\n",
    "        Fit pixel-wise univariate or multivariate OLS regression model and extract beta coefficient and t-statistic for predictor of interest (self.permute_var). \n",
    "\n",
    "        Args:\n",
    "        - pixel_df   : (pd.DataFrame) Pixel-level regression dataframe with power epochs data and behavioral regressors. DataFrame of (n_epochs, n_regressors+1). \n",
    "                                      Regressor column data must be continuous(dtype=float), discrete(dtype=int), or categorical(dtype=pd.Categorical). \n",
    "        \n",
    "        Returns:\n",
    "        - pixel_beta : (np.array) Beta coefficient for predictor of interest from pixel-wise regression. Array of 1d float (1,)\n",
    "        - pixel_tval : (np.array) Observed t-statistic for predictor of interest from pixel-wise regression. Array of 1d float (1,)\n",
    "        \"\"\"\n",
    "\n",
    "        # formula should be in form 'col_name + col_name' if col is categorical then should be 'C(col_name)'  \n",
    "        formula    = '+ '.join(['pow ~ 1 ',(' + ').join([''.join(['C(',col,')']) if pd.api.types.is_categorical_dtype(pixel_df[col])\n",
    "                            else col for col in pixel_df.columns[~pixel_df.columns.isin(['pow'])].tolist()])])\n",
    "        \n",
    "        pixel_model = smf.ols(formula,pixel_df,missing='drop').fit()\n",
    "\n",
    "        return (pixel_model.params[self.permute_var],pixel_model.tvalues[self.permute_var])\n",
    "\n",
    "    def max_tfr_cluster(self,tfr_tstats,alternative='two-sided',output='all',clust_struct=np.ones(shape=(3,3))):\n",
    "\n",
    "        \"\"\"\n",
    "        Identify time-frequency clusters of neural activity that are significantly correlated with the predictor of interest (self.permute_var). Clusters are identified \n",
    "        from neighboring pixel regression t-statistics for the predictor of interest that exceed the tcritical threshold from the alternate hypothesis. \n",
    "\n",
    "        Args:\n",
    "        - tfr_tstats       : (np.array) Pixel regression tstatistic from coefficient estimates for predictor of interest. Array of floats (n_freqs,n_times). \n",
    "        - alternative      : (str) Alternate hypothesis for t-test. Must be 'two-sided','greater', or 'less'. Default is 'two-sided'. \n",
    "        - output           : (str) Output format for max cluster statistics. Must be 'all', 'cluster_stat', or 'freq_time'. Default is 'all'.\n",
    "        - clust_struct     : (np.array) Binary matrix to specify cluster structure for scipy.ndimage.label. Array of (3,3). \n",
    "                                        Default is np.ones.shape(3,3), to allow diagonal cluster pixels (Not the scipy.ndimage.label default).\n",
    "                                        https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.label.html\n",
    "\n",
    "        Returns:\n",
    "        - max_cluster_data : (list) Beta coefficient for predictor of interest for each pixel regression. List (len=2 if 'two-sided') of dict(s).\n",
    "                                    If output = 'all', return dictionary of maximum cluster statistic ('cluster_stat' : sum of pixel t-statistics), \n",
    "                                    cluster frequency indices ('freq_idx':(freq_x,freq_y)), and cluster time indices ('time_idx':(time_x,time_y)). \n",
    "                                    If output = 'cluster_stat', return only [{cluster_stat}]. If output = 'freq_time', return only {freq_idx,time_idx}\n",
    "                                    ** If no clusters are found, max_cluster_data = {[]}\n",
    "        *** add docstring for expanded output\n",
    "        \"\"\"\n",
    "        \n",
    "        max_cluster_data = []\n",
    "        # Create binary matrix from tfr_tstats by thresholding pixel t-statistics by tcritical. (1 = pixel t-statistic exceeded tcritical threshold)\n",
    "        for binary_mat in self.threshold_tfr_tstat(tfr_tstats,alternative):\n",
    "\n",
    "            # test whether there are any pixels above tcritical threshold\n",
    "            if np.sum(binary_mat) != 0: \n",
    "                # Find clusters of pixels with t-statistics exceeding tcritical\n",
    "                cluster_label, num_clusters = label(binary_mat,clust_struct)\n",
    "                # use argmax to find index of largest absolute value of cluster t statistic sums \n",
    "                max_label = np.argmax([np.abs(np.sum(tfr_tstats[cluster_label==i+1])) for i in range(num_clusters)])+1\n",
    "                # use max_label index to compute cluster tstat sum (without absolute value)\n",
    "                max_clust_stat = np.sum(tfr_tstats[cluster_label==max_label])\n",
    "                # find 2D indices of minimum/maximum cluster frequencies and times \n",
    "                clust_freqs, clust_times = [(np.min(arr),np.max(arr)) for arr in np.where(cluster_label == max_label)]\n",
    "\n",
    "                if output == 'all':\n",
    "                    max_cluster_data.append({'cluster_stat':max_clust_stat,'freq_idx':clust_freqs,'time_idx':clust_times})\n",
    "                elif output == 'cluster_stat':\n",
    "                    max_cluster_data.append({'cluster_stat':max_clust_stat})\n",
    "                elif output == 'freq_time':\n",
    "                    max_cluster_data.append({'freq_idx':clust_freqs,'time_idx':clust_times})\n",
    "                elif output == 'expanded':\n",
    "                    max_cluster_data.append({'cluster_stat':max_clust_stat,'freq_idx':clust_freqs,'time_idx':clust_times,\n",
    "                                            'max_label':max_label,'all_clusters':cluster_label})\n",
    "            \n",
    "            else: # if there is no cluster, keep max_cluster_data empty list\n",
    "                continue\n",
    "            \n",
    "        return max_cluster_data\n",
    "\n",
    "    def compute_tcritical(self,alternative ='two-sided',alpha=0.05):\n",
    "        \"\"\"\n",
    "        Calculate critical t-values for regression model.\n",
    "        \n",
    "        Args:\n",
    "        - alternative : (str) Alternate hypothesis for t-test. Must be 'two-sided','greater', or 'less'. Default is 'two-sided'.\n",
    "        - alpha       : (float) Significance level. Default is 0.05.\n",
    "\n",
    "        Returns:\n",
    "        - tcritical   : (float) Critical t-statistic for hypothesis test. Positive value when alternative = 'two-sided' or 'greater'. Negative when alternative = 'less'. \n",
    "        \"\"\"\n",
    "\n",
    "        # Set number of tails for t-tests using 'alternative' parameter input string. \n",
    "            # tails = 2 if alternative = 'two-sided' (two tailed hypothesis test)\n",
    "            # tails = 1 if alternative = 'greater' or 'less' (one tailed hypothesis test)\n",
    "        tails = len(alternative.split('-')) \n",
    "\n",
    "        # Calculate degrees of freedom (N-k-1) \n",
    "        deg_free = float(len(self.predictor_data)-len(self.predictor_data.columns)-1) #### predictor data must only include regressors in columns\n",
    "\n",
    "        # Return tcritical from t-distribution. Significance level is alpha/2 for two tailed hypothesis tests (alternative = 'two-sided').\n",
    "        return (t.ppf(1-(alpha/tails),deg_free) if alternative != 'less' else np.negative(t.ppf(1-(alpha/tails),deg_free)))\n",
    "\n",
    "    def threshold_tfr_tstat(self,tfr_tstats,alternative='two-sided'):\n",
    "        \"\"\"\n",
    "        Threshold tfr t-statistic matrix using tcritical.\n",
    "\n",
    "        Args:\n",
    "        - tfr_tstats  : (np.array) Matrix of t-statistics from pixel-wise regressions. Array of floats (n_freqs, n_times). \n",
    "        - alternative : (str) Type of hypothesis test for t-distribution. Must be 'two-sided', 'greater', 'less'. Default is 'two-sided'.\n",
    "\n",
    "        Returns:\n",
    "        - binary_mat  : (np.array) Binary matrix results of pixel-wise t-tests. Pixel = 1 when tstatistic > tcritical, else pixel = 0. List of array(s) (n_freqs, n_times).\n",
    "        \"\"\"\n",
    "\n",
    "        if alternative == 'two-sided': \n",
    "            return [(tfr_tstats>self.compute_tcritical()).astype(int), (tfr_tstats<np.negative(self.compute_tcritical())).astype(int)]\n",
    "\n",
    "        elif alternative == 'greater':\n",
    "            return [(tfr_tstats>self.compute_tcritical(tails=1,alternative='greater')).astype(int)]\n",
    "\n",
    "        elif alternative == 'less':\n",
    "            return [(tfr_tstats<self.compute_tcritical(tails=1,alternative='less')).astype(int)] \n",
    "        else: \n",
    "            raise ValueError('Alternative hypothesis must be two-sided, greater, or less not {alternative}')\n",
    "    \n",
    "    def expand_tfr_indices(self):\n",
    "        \"\"\"\n",
    "        Create list of tfr pixel indices for parallelized tfr_regression.\n",
    "\n",
    "        Returns:\n",
    "        - iter_tup : (list) Time-frequency indices for all pixels in tfr_data. List of tuples [(freq_x_index,freq_y_index),(time_x_index,time_y_index)]        \n",
    "        \"\"\"\n",
    "\n",
    "        return list(map(tuple,np.unravel_index(np.dstack(([*np.indices(self.tfr_dims)])),np.product(self.tfr_dims)\n",
    "                            )[0].reshape(np.product(np.dstack(([*np.indices(self.tfr_dims)])).shape[:2]),-1)))\n",
    "\n",
    "    def make_pixel_df(self,epoch_data,permuted=False):\n",
    "        \"\"\"\n",
    "        Format input data for pixel regression.  input data. Make pixel-level (frequency x timepoint) dataframe. Add tfr power data for single pixel to predictor_df. \n",
    "\n",
    "        Args:\n",
    "        - epoch_data : (str) Alternate hypothesis for t-test. Must be 'two-sided','greater', or'less'. Default is 'two-sided'. Array of 1d integers or floats (n_epochs,).\n",
    "        \n",
    "        Returns:\n",
    "        - pixel_df   : (pd.DataFrame) Pixel regression input dataframe containing power epochs and task-based behavioral regressor data (dtype=int/float/pd.Categorical). \n",
    "                                      DataFrame of (n_epochs, n_regressors+1). \n",
    "        \n",
    "        ##### to-do add docstring info for permuted kwargs\n",
    "        \"\"\"\n",
    "        if permuted: ###### make clear that this permanently updates predictor data!!!!\n",
    "            self.predictor_data[self.permute_var] = np.random.permutation(self.predictor_data[self.permute_var].values)\n",
    "            return self.predictor_data.assign(pow=epoch_data)\n",
    "        else: \n",
    "            return self.predictor_data.assign(pow=epoch_data) \n",
    "\n",
    "###### UNTESTED PERMUTATION FUNCTIONS!!!\n",
    "\n",
    "    def compute_null_cluster_stats(self,num_permutations=None):\n",
    "\n",
    "        #### for every permutation:\n",
    "            # permute predictor of interest, then make pixel df \n",
    "            # run tfr regression & extract permutation t stats \n",
    "            # find max cluster statistics for permutation  \n",
    "\n",
    "        null_cluster_distribution = Parallel(n_jobs=-1, verbose=5)(delayed\n",
    "                                            (self.max_tfr_cluster(output='cluster_stat'))(self.permuted_tfr_regression) for n in num_permutations)\n",
    "        return null_cluster_distribution\n",
    "\n",
    "\n",
    "    def permuted_tfr_regression(self):\n",
    "        \"\"\"\n",
    "        Run tfr regression for single permutation\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        iter_tup = self.expand_tfr_indices()\n",
    "\n",
    "        # either precompute pixel_args before passing to parallel, or run all together in loop. - check later!! \n",
    "        perm_args = [self.make_pixel_df(self.tfr_data[:,freq_idx,time_idx],permuted=True) for freq_idx,time_idx in iter_tup]\n",
    "\n",
    "        # Run regression on permuted data + extract tstats only\n",
    "\n",
    "        # run pixel permutations in parallel \n",
    "        permuted_results = Parallel(n_jobs=-1, verbose=5)(\n",
    "                        delayed(self.pixel_regression)(args)\n",
    "                            for args in perm_args)      \n",
    "        \n",
    "        # preallocate np arrays for betas + tstats\n",
    "        perm_tstats = np.zeros((self.tfr_dims))\n",
    "\n",
    "        # expanded_results is a list of tuples (beta,tstat) for every pixel \n",
    "        for count,(freq_idx,time_idx) in enumerate(iter_tup):\n",
    "            perm_tstats[freq_idx,time_idx] = permuted_results[count][1]\n",
    "        \n",
    "        return perm_tstats\n",
    "    \n",
    "\n",
    "\n",
    "    # def cluster_significance_test(self, null_distribution,max_cluster_stat,alpha=0.05,alternative='two-sided'):\n",
    "    #     \"\"\"\n",
    "    #     Compute non-parametric pvalue from cluster permutation data \n",
    "\n",
    "    #             - alpha (float): Significance level. Default is 0.05.\n",
    "\n",
    "    #     \"\"\"\n",
    "        \n",
    "    #     return cluster_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current to-dos: \n",
    "\n",
    "- reformat max_cluster_data freq/time indices returns\n",
    "- permutation class functions \n",
    "- speed:\n",
    "    https://joblib.readthedocs.io/en/latest/parallel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed, parallel_config\n",
    "\n",
    "# with parallel_config(backend=\"loky\", inner_max_num_threads=2):\n",
    "#     results = Parallel(n_jobs=4)(delayed(func)(x, y) for x, y in data)\n",
    "\n",
    "# with parallel_config(backend='custom', endpoint='http://compute',\n",
    "#                      api_key='42'):\n",
    "#     Parallel()(delayed(some_function)(i) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def parallel_apply_along_axis(func1d, axis, arr, *args, **kwargs):\n",
    "#     \"\"\"\n",
    "#     Like numpy.apply_along_axis(), but takes advantage of multiple\n",
    "#     cores.\n",
    "#     \"\"\"        \n",
    "#     # Effective axis where apply_along_axis() will be applied by each\n",
    "#     # worker (any non-zero axis number would work, so as to allow the use\n",
    "#     # of `np.array_split()`, which is only done on axis 0):\n",
    "#     effective_axis = 1 if axis == 0 else axis\n",
    "#     if effective_axis != axis:\n",
    "#         arr = arr.swapaxes(axis, effective_axis)\n",
    "\n",
    "#     # Chunks for the mapping (only a few chunks):\n",
    "#     chunks = [(func1d, effective_axis, sub_arr, args, kwargs)\n",
    "#               for sub_arr in np.array_split(arr, multiprocessing.cpu_count())]\n",
    "\n",
    "#     pool = multiprocessing.Pool()\n",
    "#     individual_results = pool.map(unpacking_apply_along_axis, chunks)\n",
    "#     # Freeing the workers:\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "\n",
    "#     return np.concatenate(individual_results)\n",
    "\n",
    "# def unpacking_apply_along_axis(all_args):\n",
    "#     \"\"\"…\"\"\"\n",
    "#     (func1d, axis, arr, args, kwargs) = all_args\n",
    "#     …\n",
    "\n",
    "# https://stackoverflow.com/questions/45526700/easy-parallelization-of-numpy-apply-along-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/76339092/convert-joblib-parallel-to-multiprocessing-pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "# import numpy as np\n",
    "\n",
    "# def my_function(x):\n",
    "#     pass     # do something and return something\n",
    "\n",
    "# if __name__ == '__main__':    \n",
    "#     X = np.arange(6).reshape((3,2))\n",
    "#     pool = Pool(processes = 4)\n",
    "#     results = pool.map(my_function, map(lambda x: x, X))\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "\n",
    "# https://stackoverflow.com/questions/16468717/iterating-over-numpy-matrix-rows-to-apply-a-function-each\n",
    "\n",
    "# https://stackoverflow.com/questions/8079061/function-application-over-numpys-matrix-row-column\n",
    "\n",
    "# https://docs.python.org/3/library/itertools.html#itertools.filterfalse\n",
    "\n",
    "# https://medium.com/pythoneers/vectorization-in-python-an-alternative-to-python-loops-2728d6d7cd3e\n",
    "\n",
    "# https://realpython.com/python-map-function/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/pythoneers/vectorization-in-python-an-alternative-to-python-loops-2728d6d7cd3e\n",
    "# https://realpython.com/python-map-function/\n",
    "\n",
    "# https://joblib.readthedocs.io/en/latest/auto_examples/parallel_memmap.html#sphx-glr-auto-examples-parallel-memmap-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/16468717/iterating-over-numpy-matrix-rows-to-apply-a-function-each\n",
    "# from multiprocessing import Pool\n",
    "# import numpy as np\n",
    "\n",
    "# def my_function(x):\n",
    "#     pass     # do something and return something\n",
    "\n",
    "# if __name__ == '__main__':    \n",
    "#     X = np.arange(6).reshape((3,2))\n",
    "#     pool = Pool(processes = 4)\n",
    "#     results = pool.map(my_function, map(lambda x: x, X))\n",
    "#     pool.close()\n",
    "#     pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://medium.com/@nirmalya.ghosh/13-ways-to-speedup-python-loops-e3ee56cd6b73\n",
    "\n",
    "# def test_08_v1(n):\n",
    "#   # Improved version\n",
    "#   # (Efficiently calculates the nth Fibonacci\n",
    "#   # number using a generator)\n",
    "#   a, b = 0, 1\n",
    "#   for _ in range(n):\n",
    "#     yield a\n",
    "#     a, b = b, a + b\n",
    "\n",
    "\n",
    "#     def some_function_X(x):\n",
    "#   # This would normally be a function containing application logic\n",
    "#   # which required it to be made into a separate function\n",
    "#   # (for the purpose of this test, just calculate and return the square)\n",
    "#   return x**2\n",
    "\n",
    "# def test_09_v0(numbers):\n",
    "#   # Baseline version (Inefficient way)\n",
    "#   output = []\n",
    "#   for i in numbers:\n",
    "#     output.append(some_function_X(i))\n",
    "\n",
    "#   return output\n",
    "\n",
    "# def test_09_v1(numbers):\n",
    "#   # Improved version\n",
    "#   # (Using Python's built-in map() function)\n",
    "#   output = map(some_function_X, numbers)\n",
    "#   return output\n",
    "\n",
    "\n",
    "# def test_12_v0(numbers):\n",
    "#   # Baseline version (Inefficient way)\n",
    "#   filtered_data = []\n",
    "#   for i in numbers:\n",
    "#     filtered_data.extend(list(\n",
    "#         filter(lambda x: x % 5 == 0,\n",
    "#                 range(1, i**2))))\n",
    "  \n",
    "#   return filtered_data\n",
    "\n",
    "# from itertools import filterfalse\n",
    "\n",
    "\n",
    "\n",
    "# def test_12_v1(numbers):\n",
    "#   # Improved version\n",
    "#   # (using filterfalse)\n",
    "#   filtered_data = []\n",
    "#   for i in numbers:\n",
    "#     filtered_data.extend(list(\n",
    "#         filterfalse(lambda x: x % 5 != 0,\n",
    "#                     range(1, i**2))))\n",
    "    \n",
    "#     return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel(n_jobs=2, prefer=\"threads\")(\n",
    "#     delayed(sqrt)(i ** 2) for i in range(10))\n",
    "\n",
    "\n",
    "# from joblib import parallel_config\n",
    "# with parallel_config(backend='threading', n_jobs=2):\n",
    "#    Parallel()(delayed(sqrt)(i ** 2) for i in range(10))\n",
    "\n",
    "#    shared_set = set()\n",
    "# def collect(x):\n",
    "#    shared_set.add(x)\n",
    "\n",
    "# Parallel(n_jobs=2, require='sharedmem')(\n",
    "#     delayed(collect)(i) for i in range(5))\n",
    "# [None, None, None, None, None]\n",
    "\n",
    "\n",
    "# with Parallel(n_jobs=2) as parallel:\n",
    "#    accumulator = 0.\n",
    "#    n_iter = 0\n",
    "#    while accumulator < 1000:\n",
    "#        results = parallel(delayed(sqrt)(accumulator + i ** 2)\n",
    "#                           for i in range(5))\n",
    "#        accumulator += sum(results)  # synchronization barrier\n",
    "#        n_iter += 1\n",
    "\n",
    "# (accumulator, n_iter) \n",
    "# # https://joblib.readthedocs.io/en/latest/parallel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import Pool\n",
    "# from multiprocessing import Pool\n",
    "# #Define a worker — a function which will be executed in parallel\n",
    "# def worker(x):\n",
    "#  return x*x\n",
    "# #Assuming you want to use 3 processors\n",
    "# num_processors = 3\n",
    "# #Create a pool of processors\n",
    "# p=Pool(processes = num_processors)\n",
    "# #get them to work in parallel\n",
    "# output = p.map(worker,[i for i in range(0,3)])\n",
    "# print(output)\n",
    "\n",
    "# from multiprocessing import Pool\n",
    "# import workers\n",
    "# if __name__ ==  '__main__': \n",
    "#  num_processors = 3\n",
    "#  p=Pool(processes = num_processors)\n",
    "#  output = p.map(workers.worker,[i for i in range(0,3)])\n",
    "#  print(output)\n",
    "# https://medium.com/@grvsinghal/speed-up-your-code-using-multiprocessing-in-python-36e4e703213e\n",
    "# https://medium.com/@grvsinghal/speed-up-your-python-code-using-multiprocessing-on-windows-and-jupyter-or-ipython-2714b49d6fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/42220458/what-does-the-delayed-function-do-when-used-with-joblib-in-python\n",
    "\n",
    "# import joblib\n",
    "\n",
    "# @joblib.delayed\n",
    "# def getHog(image):\n",
    "#     \"\"\"Some time-consuming function on an image\"\"\"\n",
    "#     ...\n",
    "\n",
    "# # Running this in parallel\n",
    "# with joblib.Parallel(backend=\"loky\", n_jobs=8) as parallel:\n",
    "#     result = parallel(getHog(img) for img in allImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_function(x):\n",
    "#     \"\"\"The function you want to compute in parallel.\"\"\"\n",
    "#     x += 1\n",
    "#     return x\n",
    "\n",
    "# import multiprocessing\n",
    "# import workers\n",
    "\n",
    "# pool = multiprocessing.Pool()\n",
    "# results = pool.map(workers.my_function, [1,2,3,4,5,6])\n",
    "# # print(results)\n",
    "# # \n",
    "# # https://stackoverflow.com/questions/23641475/multiprocessing-working-in-python-but-not-in-ipython/23641560#23641560\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "power_epochs.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = [(np.squeeze(power_epochs._data[:,ch_ix,:,:].copy()),\n",
    "#                             predictor_data,permute_var,ch_name) for ch_ix, ch_name\n",
    "#                             in enumerate(power_epochs.ch_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST PERMUTATIONS \n",
    "start = time.time() # start timer\n",
    "\n",
    "\n",
    "args = [(np.squeeze(power_epochs._data[:,ch_ix,:,:].copy()),\n",
    "         predictor_data,permute_var,ch_name) for ch_ix, ch_name\n",
    "         in enumerate(power_epochs.ch_names)]\n",
    "\n",
    "# Perform permutations in parallel\n",
    "elec_Cluster_objs = Parallel(n_jobs=-1, verbose=5)(\n",
    "delayed(TFR_Cluster_Test)(*a)\n",
    "for a in args)\n",
    "\n",
    "#         pickle.dump(elec_permuted_data, open(f'{results_dir}{subj_id}_{c}_perm_clusters{date}.pkl', \"wb\")) \n",
    "\n",
    "end = time.time()    \n",
    "print('{:.4f} s'.format(end-start)) # print time elapsed for computation (approx 4 seconds per permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time() # start timer\n",
    "\n",
    "## run simple linear regression on electrodes in parallel to speed up computation - I did this for just the subset OFC electrodes.\n",
    "subj_all_elec_data = Parallel(n_jobs=-1,verbose=5)(\n",
    "    delayed(elec_obj.tfr_cluster_results())(num_permutations=None\n",
    "        ) for elec_obj in elec_Cluster_objs)\n",
    "\n",
    "end = time.time()    \n",
    "print('{:.4f} s'.format(end-start)) # print time elapsed for computation (approx 20 seconds per channel)\n",
    "\n",
    "# save subj cluster data for all electrodes\n",
    "# pickle.dump(subj_all_elec_data, open(f'{save_dir}{subj_id}_all_elec_real_clusters.pkl', \"wb\")) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time() # start timer\n",
    "\n",
    "# ## run simple linear regression on electrodes in parallel to speed up computation - I did this for just the subset OFC electrodes.\n",
    "# subj_all_elec_data = Parallel(n_jobs=-1,verbose=5)(\n",
    "#     delayed(TFR_Cluster_Test.tfr_cluster_results())(\n",
    "#         np.squeeze(power_epochs._data[:,ch_ix,:,:].copy()),predictor_data,permute_var,ch_name\n",
    "#         ) for ch_ix, ch_name in enumerate(power_epochs.ch_names))\n",
    "\n",
    "# end = time.time()    \n",
    "# print('{:.4f} s'.format(end-start)) # print time elapsed for computation (approx 20 seconds per channel)\n",
    "\n",
    "# # save subj cluster data for all electrodes\n",
    "# pickle.dump(subj_all_elec_data, open(f'{save_dir}{subj_id}_all_elec_real_clusters.pkl', \"wb\")) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_test  = TFR_Cluster_Test(tfr_data,beh_df,permute_var,'laims2-laims3')\n",
    "betas, tstats = cluster_test.tfr_regression()\n",
    "cluster_data  = cluster_test.max_tfr_cluster(tstats)\n",
    "\n",
    "cluster_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perm = TFR_Cluster_Test(tfr_data,beh_df,permute_var,'laims2-laims3').tfr_cluster_results(num_permutations=10)\n",
    "test_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/tutorial/classes.html#generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols_dmatrix = pd.get_dummies(predictor_data,drop_first=True)\n",
    "# permute_var_idx = np.where(ols_dmatrix.columns  == 'decisionCPE')[0][0]\n",
    "# epoch_data = tfr_data[:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# excluded_args = inspect.getargs(solo_pixel_regression.__code__).args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]],\n",
       "\n",
       "       [[ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True],\n",
       "        [ True,  True,  True, ...,  True,  True,  True]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.resize(np.resize(tfr_data,(150,45030)),tfr_data.shape) == tfr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import inspect\n",
    "# excluded_args = inspect.getargs(solo_pixel_regression.__code__).args\n",
    "\n",
    "# test_vecfunct = np.vectorize(solo_pixel_regression,excluded=excluded_args.remove('epoch_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time() # start timer\n",
    "\n",
    "# # # formula should be in form 'col_name + col_name' if col is categorical then should be 'C(col_name)'  \n",
    "# # formula    = '+ '.join(['pow ~ 1 ',(' + ').join([''.join(['C(',col,')']) if pd.api.types.is_categorical_dtype(pixel_df[col])\n",
    "# #                     else col for col in pixel_df.columns[~pixel_df.columns.isin(['pow'])].tolist()])])\n",
    "\n",
    "# # pixel_model = smf.ols(formula,pixel_df,missing='drop').fit()\n",
    "# # smf.ols(formula,pd.get_dummies(pixel_df),missing='drop').fit()\n",
    "# pixel_model = sm.OLS(tfr_data[:,0,0],sm.add_constant(pd.get_dummies(pixel_df,drop_first=True).to_numpy()),missing='drop').fit()\n",
    "# print((pixel_model.params,pixel_model.tvalues))\n",
    "\n",
    "# end = time.time()    \n",
    "# print('{:.4f} s'.format(end-start)) # print time elapsed for computation (approx 20 seconds per channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # np.apply_over_axes(test_vecfunct, tfr_data, [0],ols_dmatrix,permute_var_idx).reshape(tfr_data.shape[1],tfr_data.shape[2])\n",
    "# # np.apply_over_axes(lambda x: test_vecfunct(ols_dmatrix,permute_var_idx,x), tfr_data, [0]).reshape(tfr_data.shape[1],tfr_data.shape[2])\n",
    "\n",
    "# # https://blog.enterprisedna.co/how-to-map-a-function-over-numpy-array/# pixel_df.GambleChoice\n",
    "# # pd.get_dummies(pixel_df.GambleChoice,drop_first=True).to_numpy().flatten()\n",
    "# # np.asarray(pd.get_dummies(pixel_df.GambleChoice,drop_first=True))\n",
    "\n",
    "# dmat = [pd.get_dummies(pixel_df.GambleChoice,drop_first=True).to_numpy().flatten() else data.values\n",
    "#     data.values if  for _,data in pixel_df.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Extract Surrogate Clusters from Pixel-wise Permutation\n",
    "- For loop for each electrode- \n",
    "- Run each permutation (1000x) in parallel within electrode loop\n",
    "- Calculate max cluster p value for each +/- cluster for each electrode\n",
    "- Save permuted cluster statistics for each electrode \n",
    "\n",
    "DEPENDENCIES: permuted_tfr_cluster_test, tfr_cluster_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perm = TFR_Cluster_Test(tfr_data,beh_df,permute_var,'laims2-laims3').tfr_cluster_results(num_permutations=10)\n",
    "test_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_cluster_results = TFR_Cluster_Test(tfr_data,beh_df,permute_var,'laims2-laims3').tfr_cluster_results(num_permutations=200)\n",
    "perm_cluster_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_cluster_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_full_perm = TFR_Cluster_Test(tfr_data,beh_df,permute_var,ch_name).tfr_cluster_results(num_permutations=200)\n",
    "# test_full_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from scipy.ndimage import label \n",
    "# from scipy.stats import  t\n",
    "# from joblib import Parallel, delayed\n",
    "# import statsmodels.api as sm \n",
    "# from scipy.ndimage import label \n",
    "# import statsmodels.formula.api as smf\n",
    "# import tqdm\n",
    "# from scipy.ndimage import label \n",
    "# import time\n",
    "\n",
    "\n",
    "\n",
    "class TFR_Cluster_Test(object):\n",
    "    '''\n",
    "    Single-electrode neurophysiology object class to identify time-frequency resolved neural activity correlates of complex behavioral variables using non-parametric \n",
    "    cluster-based permutation testing.   \n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    tfr_data       : (np.array) Single electrode tfr data matrix. Array of floats (n_epochs,n_freqs,n_times). \n",
    "    tfr_dims       : (tuple) Frequency and time dimensions of tfr_data. Tuple of integers (n_freq,n_times). \n",
    "    ch_name        : (str) Unique electrode identification label. String of characters.\n",
    "    predictor_data : (pd.DataFrame) Regressors from task behavior with continuous, discreet, or categorical data. DataFrame of (rows=n_epochs,columns=n_regressors). \n",
    "    permute_var    : (str) Column label for primary regressor of interest.\n",
    "      \n",
    "    Methods\n",
    "    ----------\n",
    "    **To-do: fill in methods info\n",
    "    '''\n",
    "\n",
    "    def __init__(self, tfr_data, predictor_data, permute_var, ch_name, **kwargs):\n",
    "        '''\n",
    "        Args:\n",
    "        - tfr_data       : (np.array) Single electrode tfr data matrix. Array of floats (n_epochs,n_freqs,n_times). \n",
    "        - predictor_data : (pd.DataFrame) Task-based regressor data with dtypes continuous/discreet(int64/float) or categorical(pd.Categorical). DataFrame of (n_epochs,n_regressors).\n",
    "        - permute_var    : (str) Column label for primary regressor of interest. Array of 1d integers or floats (n_epochs,).\n",
    "        - ch_name        : (str) Unique electrode identification label. String of characters.  \n",
    "        - **kwargs       : (optional) alternative, alpha, cluster_shape\n",
    "        '''\n",
    "\n",
    "        self.tfr_data       = tfr_data  # single electrode tfr data\n",
    "        self.predictor_data = predictor_data # single subject behav data\n",
    "        self.tfr_dims       = self.tfr_data.shape[1:] # time-frequency dims of electrode data (n_freqs x n_times)\n",
    "        self.permute_var    = permute_var # variable to permute in regression model \n",
    "        self.ch_name        = ch_name # channel name for single electrode tfr data\n",
    "\n",
    "    def tfr_regression(self):\n",
    "        '''\n",
    "        Performs univariate or multivariate OLS regression across tfr matrix for all pixel-level time-frequency power data and task-based predictor variables. Regressions are parallelized across pixels.\n",
    "\n",
    "        Returns:\n",
    "        - tfr_betas  : (np.array) Matrix of beta coefficients for predictor of interest for each pixel regression. Array of (n_freqs,n_times). \n",
    "        - tfr_tstats : (np.array) Matrix of t-statistics from coefficient estimates for predictor of interest for each pixel regression. Array of (n_freqs,n_times). \n",
    "        '''\n",
    "        \n",
    "        # Prepare arguments for parallelization`using tfr matrix indices converted to list of tuples (freq x power)\n",
    "        pixel_args = [self.make_pixel_df(self.tfr_data[:,freq_idx,time_idx]) for freq_idx,time_idx in self.expand_tfr_indices()]\n",
    "        \n",
    "        # run pixel permutations in parallel \n",
    "        expanded_results = Parallel(n_jobs=-1, verbose=5)(\n",
    "                        delayed(self.pixel_regression)(args)\n",
    "                            for args in pixel_args)      \n",
    "\n",
    "        # preallocate np arrays for betas + tstats\n",
    "        tfr_betas  = np.zeros((self.tfr_dims))\n",
    "        tfr_tstats = np.zeros((self.tfr_dims))\n",
    "\n",
    "        # expanded_results is a list of tuples (beta,tstat) for every pixel \n",
    "        for count,(freq_idx,time_idx) in enumerate(self.expand_tfr_indices()):\n",
    "            tfr_betas[freq_idx,time_idx]  = expanded_results[count][0]\n",
    "            tfr_tstats[freq_idx,time_idx] = expanded_results[count][1]\n",
    "        \n",
    "        return tfr_betas, tfr_tstats\n",
    "\n",
    "    def pixel_regression(self,pixel_df):\n",
    "        '''        \n",
    "        Fit pixel-wise univariate or multivariate OLS regression model and extract beta coefficient and t-statistic for predictor of interest (self.permute_var). \n",
    "\n",
    "        Args:\n",
    "        - pixel_df   : (pd.DataFrame) Pixel-level regression dataframe with power epochs data and behavioral regressors. DataFrame of (n_epochs, n_regressors+1). \n",
    "                                      Regressor column data must be continuous(dtype=float), discrete(dtype=int), or categorical(dtype=pd.Categorical). \n",
    "        \n",
    "        Returns:\n",
    "        - pixel_beta : (np.array) Beta coefficient for predictor of interest from pixel-wise regression. Array of 1d float (1,)\n",
    "        - pixel_tval : (np.array) Observed t-statistic for predictor of interest from pixel-wise regression. Array of 1d float (1,)\n",
    "        '''\n",
    "\n",
    "\n",
    "        # formula should be in form 'col_name + col_name' if col is categorical then should be 'C(col_name)'  \n",
    "        formula    = '+ '.join(['pow ~ 1 ',(' + ').join([''.join(['C(',col,')']) if pd.api.types.is_categorical_dtype(pixel_df[col])\n",
    "                            else col for col in pixel_df.columns[~pixel_df.columns.isin(['pow'])].tolist()])])\n",
    "        \n",
    "        pixel_model = smf.ols(formula,pixel_df,missing='drop').fit()\n",
    "\n",
    "        return (pixel_model.params[self.permute_var],pixel_model.tvalues[self.permute_var])\n",
    "\n",
    "    def max_tfr_cluster(self,tfr_tstats,alternative='two-sided',output='all',clust_struct=np.ones(shape=(3,3))):\n",
    "\n",
    "        '''\n",
    "        Identify time-frequency clusters of neural activity that are significantly correlated with the predictor of interest (self.permute_var). Clusters are identified \n",
    "        from neighboring pixel regression t-statistics for the predictor of interest that exceed the tcritical threshold from the alternate hypothesis. \n",
    "\n",
    "        Args:\n",
    "        - tfr_tstats       : (np.array) Pixel regression tstatistic from coefficient estimates for predictor of interest. Array of floats (n_freqs,n_times). \n",
    "        - alternative      : (str) Alternate hypothesis for t-test. Must be 'two-sided','greater', or 'less'. Default is 'two-sided'. \n",
    "        - output           : (str) Output format for max cluster statistics. Must be 'all', 'cluster_stat', or 'freq_time'. Default is 'all'.\n",
    "        - clust_struct     : (np.array) Binary matrix to specify cluster structure for scipy.ndimage.label. Array of (3,3). \n",
    "                                        Default is np.ones.shape(3,3), to allow diagonal cluster pixels (Not the scipy.ndimage.label default).\n",
    "                                        https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.label.html\n",
    "\n",
    "        Returns:\n",
    "        - max_cluster_data : (list) Beta coefficient for predictor of interest for each pixel regression. List (len=2 if 'two-sided') of dict(s).\n",
    "                                    If output = 'all', return dictionary of maximum cluster statistic ('cluster_stat' : sum of pixel t-statistics), \n",
    "                                    cluster frequency indices ('freq_idx':(freq_x,freq_y)), and cluster time indices ('time_idx':(time_x,time_y)). \n",
    "                                    If output = 'cluster_stat', return only [{cluster_stat}]. If output = 'freq_time', return only {freq_idx,time_idx}\n",
    "                                    ** If no clusters are found, max_cluster_data contains list of empty dictionaries\n",
    "        *** add docstring for expanded output\n",
    "        '''\n",
    "        \n",
    "        max_cluster_data = []\n",
    "        # Create binary matrix from tfr_tstats by thresholding pixel t-statistics by tcritical. (1 = pixel t-statistic exceeded tcritical threshold)\n",
    "        for binary_mat in self.threshold_tfr_tstat(tfr_tstats,alternative):\n",
    "\n",
    "            # test whether there are any pixels above tcritical threshold\n",
    "            if np.sum(binary_mat) != 0: \n",
    "                # Find clusters of pixels with t-statistics exceeding tcritical\n",
    "                cluster_label, num_clusters = label(binary_mat,clust_struct)\n",
    "                # use argmax to find index of largest absolute value of cluster t statistic sums \n",
    "                max_label = np.argmax([np.abs(np.sum(tfr_tstats[cluster_label==i+1])) for i in range(num_clusters)])+1\n",
    "                # use max_label index to compute cluster tstat sum (without absolute value)\n",
    "                max_clust_stat = np.sum(tfr_tstats[cluster_label==max_label])\n",
    "                # find 2D indices of minimum/maximum cluster frequencies and times \n",
    "                clust_freqs, clust_times = [(np.min(arr),np.max(arr)) for arr in np.where(cluster_label == max_label)]\n",
    "\n",
    "                if output == 'all':\n",
    "                    max_cluster_data.append({'cluster_stat':max_clust_stat,'freq_idx':clust_freqs,'time_idx':clust_times})\n",
    "                elif output == 'cluster_stat':\n",
    "                    max_cluster_data.append({'cluster_stat':max_clust_stat})\n",
    "                elif output == 'freq_time':\n",
    "                    max_cluster_data.append({'freq_idx':clust_freqs,'time_idx':clust_times})\n",
    "                elif output == 'expanded':\n",
    "                    max_cluster_data.append({'cluster_stat':max_clust_stat,'freq_idx':clust_freqs,'time_idx':clust_times,\n",
    "                                            'max_label':max_label,'all_clusters':cluster_label})\n",
    "            \n",
    "            else: # if there is no cluster, return max_cluster_data with empty dictionaries\n",
    "                if output == 'all':\n",
    "                    max_cluster_data.append({'cluster_stat':0,'freq_idx':0,'time_idx':0})\n",
    "                elif output == 'cluster_stat':\n",
    "                    max_cluster_data.append({'cluster_stat':0})\n",
    "                elif output == 'freq_time':\n",
    "                    max_cluster_data.append({'freq_idx':0,'time_idx':0})\n",
    "                elif output == 'expanded':\n",
    "                    max_cluster_data.append({'cluster_stat':0,'freq_idx':0,'time_idx':0,'max_label':0,'all_clusters':0})            \n",
    "        \n",
    "        return max_cluster_data\n",
    "\n",
    "    def compute_tcritical(self,alternative ='two-sided',alpha=0.05):\n",
    "        '''\n",
    "        Calculate critical t-values for regression model.\n",
    "        \n",
    "        Args:\n",
    "        - alternative : (str) Alternate hypothesis for t-test. Must be 'two-sided','greater', or 'less'. Default is 'two-sided'.\n",
    "        - alpha       : (float) Significance level. Default is 0.05.\n",
    "\n",
    "        Returns:\n",
    "        - tcritical   : (float) Critical t-statistic for hypothesis test. Positive value when alternative = 'two-sided' or 'greater'. Negative when alternative = 'less'. \n",
    "        '''\n",
    "\n",
    "        # Set number of tails for t-tests using 'alternative' parameter input string. \n",
    "            # tails = 2 if alternative = 'two-sided' (two tailed hypothesis test)\n",
    "            # tails = 1 if alternative = 'greater' or 'less' (one tailed hypothesis test)\n",
    "        tails = len(alternative.split('-')) \n",
    "\n",
    "        # Calculate degrees of freedom (N-k-1) \n",
    "        deg_free = float(len(self.predictor_data)-len(self.predictor_data.columns)-1) #### predictor data must only include regressors in columns\n",
    "\n",
    "        # Return tcritical from t-distribution. Significance level is alpha/2 for two tailed hypothesis tests (alternative = 'two-sided').\n",
    "        return (t.ppf(1-(alpha/tails),deg_free) if alternative != 'less' else np.negative(t.ppf(1-(alpha/tails),deg_free)))\n",
    "\n",
    "    def threshold_tfr_tstat(self,tfr_tstats,alternative='two-sided'):\n",
    "        '''\n",
    "        Threshold tfr t-statistic matrix using tcritical.\n",
    "\n",
    "        Args:\n",
    "        - tfr_tstats  : (np.array) Matrix of t-statistics from pixel-wise regressions. Array of floats (n_freqs, n_times). \n",
    "        - alternative : (str) Type of hypothesis test for t-distribution. Must be 'two-sided', 'greater', 'less'. Default is 'two-sided'.\n",
    "\n",
    "        Returns:\n",
    "        - binary_mat  : (np.array) Binary matrix results of pixel-wise t-tests. Pixel = 1 when tstatistic > tcritical, else pixel = 0. List of array(s) (n_freqs, n_times).\n",
    "        '''\n",
    "\n",
    "        if alternative == 'two-sided': \n",
    "            return [(tfr_tstats>self.compute_tcritical()).astype(int), (tfr_tstats<np.negative(self.compute_tcritical())).astype(int)]\n",
    "\n",
    "        elif alternative == 'greater':\n",
    "            return [(tfr_tstats>self.compute_tcritical(tails=1,alternative='greater')).astype(int)]\n",
    "\n",
    "        elif alternative == 'less':\n",
    "            return [(tfr_tstats<self.compute_tcritical(tails=1,alternative='less')).astype(int)] \n",
    "        else: \n",
    "            raise ValueError('Alternative hypothesis must be two-sided, greater, or less not {alternative}')\n",
    "    \n",
    "    def expand_tfr_indices(self):\n",
    "        '''\n",
    "        Create list of tfr pixel indices for parallelized tfr_regression.\n",
    "\n",
    "        Returns:\n",
    "        - iter_tup : (list) Time-frequency indices for all pixels in tfr_data. List of tuples [(freq_x_index,freq_y_index),(time_x_index,time_y_index)]        \n",
    "        '''\n",
    "\n",
    "        return list(map(tuple,np.unravel_index(np.dstack(([*np.indices(self.tfr_dims)])),np.product(self.tfr_dims)\n",
    "                            )[0].reshape(np.product(np.dstack(([*np.indices(self.tfr_dims)])).shape[:2]),-1)))\n",
    "\n",
    "    def make_pixel_df(self,epoch_data,permuted=False):\n",
    "        '''\n",
    "        Format input data for pixel regression.  input data. Make pixel-level (frequency x timepoint) dataframe. Add tfr power data for single pixel to predictor_df. \n",
    "\n",
    "        Args:\n",
    "        - epoch_data : (str) Alternate hypothesis for t-test. Must be 'two-sided','greater', or'less'. Default is 'two-sided'. Array of 1d integers or floats (n_epochs,).\n",
    "        \n",
    "        Returns:\n",
    "        - pixel_df   : (pd.DataFrame) Pixel regression input dataframe containing power epochs and task-based behavioral regressor data (dtype=int/float/pd.Categorical). \n",
    "                                      DataFrame of (n_epochs, n_regressors+1). \n",
    "        \n",
    "        ##### to-do add docstring info for permuted kwargs\n",
    "        '''\n",
    "        \n",
    "        if permuted: ###### make clear that this permanently updates predictor data!!!!\n",
    "            self.predictor_data[self.permute_var] = np.random.permutation(self.predictor_data[self.permute_var].values)\n",
    "            return self.predictor_data.assign(pow=epoch_data)\n",
    "        else: \n",
    "            return self.predictor_data.assign(pow=epoch_data) \n",
    "\n",
    "###### UNTESTED PERMUTATION FUNCTIONS!!!\n",
    "\n",
    "    def compute_null_cluster_stats(self,num_permutations=None):\n",
    "\n",
    "        '''\n",
    "        To-do add docstring & test\n",
    "\n",
    "        Args:\n",
    "        - null_cluster_distribution : (list) \n",
    "        '''\n",
    "\n",
    "        #### for every permutation:\n",
    "            # permute predictor of interest, then make pixel df \n",
    "            # run tfr regression & extract permutation t stats \n",
    "            # find max cluster statistics for permutation  \n",
    "\n",
    "        null_cluster_distribution = Parallel(n_jobs=-1, verbose=5)(delayed\n",
    "                                            (self.max_tfr_cluster(output='cluster_stat'))(self.permuted_tfr_regression) for n in num_permutations)\n",
    "        return null_cluster_distribution\n",
    "\n",
    "\n",
    "    def permuted_tfr_regression(self):\n",
    "        '''\n",
    "        Run tfr regression for single permutation\n",
    "        \n",
    "        Args:\n",
    "        - perm_tstats : (np.array)\n",
    "\n",
    "        '''\n",
    "\n",
    "        iter_tup = self.expand_tfr_indices()\n",
    "\n",
    "        # either precompute pixel_args before passing to parallel, or run all together in loop. - check later!! \n",
    "        perm_args = [self.make_pixel_df(self.tfr_data[:,freq_idx,time_idx],permuted=True) for freq_idx,time_idx in iter_tup]\n",
    "\n",
    "        # Run regression on permuted data + extract tstats only\n",
    "\n",
    "        # run pixel permutations in parallel \n",
    "        permuted_results = Parallel(n_jobs=-1, verbose=5)(\n",
    "                        delayed(self.pixel_regression)(args)\n",
    "                            for args in perm_args)      \n",
    "        \n",
    "        # preallocate np arrays for betas + tstats\n",
    "        perm_tstats = np.zeros((self.tfr_dims))\n",
    "\n",
    "        # expanded_results is a list of tuples (beta,tstat) for every pixel \n",
    "        for count,(freq_idx,time_idx) in enumerate(iter_tup):\n",
    "            perm_tstats[freq_idx,time_idx] = permuted_results[count][1]\n",
    "        \n",
    "        return perm_tstats\n",
    "\n",
    "    # def cluster_significance_test(self, null_distribution,max_cluster_stat,alpha=0.05,alternative='two-sided'):\n",
    "        '''\n",
    "        Compute non-param etric pvalue from cluster permutation data \n",
    "        \n",
    "        Args:\n",
    "         - alpha (float): Significance level. Default is 0.05.\n",
    "         \n",
    "        '''\n",
    "        # null_df = pd.concat([pd.DataFrame(dict,index=[0]) for dict in null_distribution]).reset_index(drop=True)\n",
    "        # null_df['sign'] = ['positive' if row.cluster_stat > 0 else 'negative' for row in null_df.iterrows()]\n",
    "        # for sign in null_df.sign.unique(): #### one loop option \n",
    "        # for cluster in max_cluster_stat: ### another loop option\n",
    "        #     null_max_clusters = null_df.cluster_stat[null_df.sign == sign]\n",
    "\n",
    "        \n",
    "    #     return cluster_pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize list to store cluster data\n",
    "# cluster_list = []\n",
    "\n",
    "# for p in range(1000):\n",
    "#     uni_test = TFR_Cluster_Test(tfr_data,pd.DataFrame(test_univar),permute_var,1000)\n",
    "#     _, uni_tstats = uni_test.tfr_regression()\n",
    "#     cluster_data = uni_test.max_tfr_cluster(uni_tstats,output='cluster_stat') \n",
    "#     # add permutation number to cluster data\n",
    "#     cluster_data['perm_num'] = p\n",
    "#     del uni_test, uni_tstats # clear memory\n",
    "#     cluster_list.append(cluster_data) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TEST PERMUTATIONS \n",
    "# num_permutations = 1000\n",
    "# start = time.time() # start timer\n",
    "\n",
    "# all_ch_perm = {}\n",
    "\n",
    "# for c in range(num_channels):\n",
    "#         ch_start = time.time() # start timer\n",
    "\n",
    "#         # Prepare arguments for the permutation function\n",
    "#         permutation_args = [\n",
    "#         (np.squeeze(power_epochs._data[:,c,:,:]), reg_data, tcritical)\n",
    "#         for _ in range(num_permutations)]\n",
    "    \n",
    "#         # Perform permutations in parallel\n",
    "#         elec_permuted_data = Parallel(n_jobs=-1, verbose=12)(\n",
    "#         delayed(permuted_tfr_cluster_test)(*args)\n",
    "#         for args in permutation_args)\n",
    "        \n",
    "#         # save in all elec dict \n",
    "#         all_ch_perm[ch_names[c]] = elec_permuted_data\n",
    "#         pickle.dump(elec_permuted_data, open(f'{results_dir}{subj_id}_{ch_names[c]}_perm_clusters.pkl', \"wb\")) \n",
    "\n",
    "#         ch_end = time.time() \n",
    "#         print(f'{ch_names[c]} permute time: ', '{:.2f}'.format(ch_end-ch_start))\n",
    "        \n",
    "        \n",
    "\n",
    "# end = time.time()    \n",
    "# print('{:.2f} s'.format(end-start)) # print time elapsed for computation (approx 4 seconds per permutation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_permutations = 1000\n",
    "# ch_start = time.time() # start timer\n",
    "\n",
    "# # Prepare arguments for the permutation function\n",
    "# permutation_args = [\n",
    "# (np.squeeze(power_epochs._data[:,c,:,:]), reg_data, tcritical)\n",
    "# for _ in range(num_permutations)]\n",
    "\n",
    "# # Perform permutations in parallel\n",
    "# elec_permuted_data_reduc = Parallel(n_jobs=-1, verbose=12)(\n",
    "# delayed(permuted_tfr_cluster_test)(*args)\n",
    "# for args in permutation_args)\n",
    "\n",
    "# # save in all elec dict \n",
    "# # all_ch_perm[ch_names[c]] = elec_permuted_data\n",
    "# pickle.dump(elec_permuted_data_reduc, open(f'{results_dir}{subj_id}_{ch_names[c]}_reduced_output_perm_clusters.pkl', \"wb\")) \n",
    "\n",
    "# ch_end = time.time() \n",
    "# print(f'{ch_names[c]} permute time: ', '{:.2f}'.format(ch_end-ch_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To implement FDR correction: \n",
    "# https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html\n",
    "# multitest.multipletests(p_upper, method='fdr_bh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hasattr(gen_tstats, '__iter__')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa4a0160430>, <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa4a01602e0>, <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa4a0160890>, <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa4a0160580>, <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa4a01603c0>, <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa4a01606d0>, <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa4a0160740>, <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa4a01609e0>, <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa4a0160970>, <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa4a01607b0>]\n"
     ]
    }
   ],
   "source": [
    "print(gen_tstats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa5e3ffe9d0>,\n",
       " <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa5e3ffea40>,\n",
       " <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa5e3ffeab0>,\n",
       " <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa5e3ffeb20>,\n",
       " <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa5e3ffeb90>,\n",
       " <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa5e3ffec00>,\n",
       " <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa5e3ffec70>,\n",
       " <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa5e3ffece0>,\n",
       " <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa5e3ffed50>,\n",
       " <generator object TFR_Cluster_Test.permuted_tfr_regression at 0x7fa5e3ffedc0>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list(gen_tstats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m perm_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "perm_t = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_test.max_tfr_cluster(output='cluster_stat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x7fa5e3ffe960>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gen_tstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TFR_Cluster_Test.max_tfr_cluster() missing 1 required positional argument: 'tfr_tstats'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/swb_ephys/lib/python3.10/site-packages/joblib/parallel.py:862\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n",
      "\u001b[1;32m    861\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;32m--> 862\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ready_batches\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n",
      "\u001b[1;32m    864\u001b[0m     \u001b[38;5;66;03m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n",
      "\u001b[1;32m    865\u001b[0m     \u001b[38;5;66;03m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m    868\u001b[0m     \u001b[38;5;66;03m# accordingly to distribute evenly the last items between all\u001b[39;00m\n",
      "\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# workers.\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/swb_ephys/lib/python3.10/queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n",
      "\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n",
      "\u001b[0;32m--> 168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\n",
      "\u001b[0;31mEmpty\u001b[0m: \n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m null_cluster_distribution \u001b[38;5;241m=\u001b[39m \u001b[43mcluster_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_null_cluster_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_permutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[0;32m~/Documents/GraduateSchool/SaezLab/NeuroCluster/NeuroCluster/NeuroCluster/scripts/tfr_cluster_test.py:252\u001b[0m, in \u001b[0;36mTFR_Cluster_Test.compute_null_cluster_stats\u001b[0;34m(self, num_permutations)\u001b[0m\n",
      "\u001b[1;32m    240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n",
      "\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03mTo-do add docstring & test\u001b[39;00m\n",
      "\u001b[1;32m    242\u001b[0m \n",
      "\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n",
      "\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m- null_cluster_distribution : (list) \u001b[39;00m\n",
      "\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m#### for every permutation:\u001b[39;00m\n",
      "\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# permute predictor of interest, then make pixel df \u001b[39;00m\n",
      "\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# run tfr regression & extract permutation t stats \u001b[39;00m\n",
      "\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# find max cluster statistics for permutation  \u001b[39;00m\n",
      "\u001b[0;32m--> 252\u001b[0m null_cluster_distribution \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\n",
      "\u001b[1;32m    253\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_tfr_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcluster_stat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermuted_tfr_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_permutations\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m null_cluster_distribution\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/swb_ephys/lib/python3.10/site-packages/joblib/parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n",
      "\u001b[1;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n",
      "\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n",
      "\u001b[0;32m   (...)\u001b[0m\n",
      "\u001b[1;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n",
      "\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n",
      "\u001b[1;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n",
      "\u001b[1;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/swb_ephys/lib/python3.10/site-packages/joblib/parallel.py:873\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n",
      "\u001b[1;32m    870\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_effective_n_jobs\n",
      "\u001b[1;32m    871\u001b[0m big_batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m n_jobs\n",
      "\u001b[0;32m--> 873\u001b[0m islice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbig_batch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(islice) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;32m    875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\n",
      "File \u001b[0;32m~/Documents/GraduateSchool/SaezLab/NeuroCluster/NeuroCluster/NeuroCluster/scripts/tfr_cluster_test.py:253\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n",
      "\u001b[1;32m    240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n",
      "\u001b[1;32m    241\u001b[0m \u001b[38;5;124;03mTo-do add docstring & test\u001b[39;00m\n",
      "\u001b[1;32m    242\u001b[0m \n",
      "\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n",
      "\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m- null_cluster_distribution : (list) \u001b[39;00m\n",
      "\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n",
      "\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m#### for every permutation:\u001b[39;00m\n",
      "\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# permute predictor of interest, then make pixel df \u001b[39;00m\n",
      "\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# run tfr regression & extract permutation t stats \u001b[39;00m\n",
      "\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# find max cluster statistics for permutation  \u001b[39;00m\n",
      "\u001b[1;32m    252\u001b[0m null_cluster_distribution \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)(delayed\n",
      "\u001b[0;32m--> 253\u001b[0m                                     (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_tfr_cluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcluster_stat\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermuted_tfr_regression()) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_permutations))\n",
      "\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m null_cluster_distribution\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: TFR_Cluster_Test.max_tfr_cluster() missing 1 required positional argument: 'tfr_tstats'"
     ]
    }
   ],
   "source": [
    "null_cluster_distribution = cluster_test.compute_null_cluster_stats(num_permutations=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to try generators: https://joblib.readthedocs.io/en/latest/auto_examples/parallel_generator.html#sphx-glr-auto-examples-parallel-generator-py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_null = []\n",
    "for gen in gen_tstats:\n",
    "    perm_max = cluster_test.max_tfr_cluster(next(gen),output='cluster_stat')\n",
    "    test_null.append(perm_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_tstats = (cluster_test.permuted_tfr_regression() for _ in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permuted_tfr_regression(self):\n",
    "    '''\n",
    "    Run tfr regression for single permutation\n",
    "    \n",
    "    Args:\n",
    "    - perm_tstats : (np.array)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    permuted_results = Parallel(n_jobs=-1, verbose=5)(delayed(self.pixel_regression)(pixel_data,permuted=True)\n",
    "                                                        for pixel_data in np.resize(self.tfr_data,(self.tfr_data.shape[0],np.prod(self.tfr_dims))).T) \n",
    "    \n",
    "    _,perm_tstats = list(zip(*permuted_results))\n",
    "    \n",
    "    return np.resize(np.array(perm_tstats), (self.tfr_data.shape[1],self.tfr_data.shape[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 8688 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 40944 tasks      | elapsed:    6.3s\n",
      "[Parallel(n_jobs=-1)]: Done 45030 out of 45030 | elapsed:    6.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 6384 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 22512 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 43248 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 45030 out of 45030 | elapsed:    6.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 8688 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 40944 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 45030 out of 45030 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 8688 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 40944 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 45030 out of 45030 | elapsed:    6.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 8688 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 40944 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done 45030 out of 45030 | elapsed:    5.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 8688 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 40944 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=-1)]: Done 45030 out of 45030 | elapsed:    6.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 8688 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 40944 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 45030 out of 45030 | elapsed:    6.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 6384 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 22512 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 43248 tasks      | elapsed:    6.9s\n",
      "[Parallel(n_jobs=-1)]: Done 45030 out of 45030 | elapsed:    7.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 6384 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 22512 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=-1)]: Done 43248 tasks      | elapsed:    6.2s\n",
      "[Parallel(n_jobs=-1)]: Done 45030 out of 45030 | elapsed:    6.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 4208 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 13296 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 44520 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 45030 out of 45030 | elapsed:    8.7s finished\n"
     ]
    }
   ],
   "source": [
    "test_null = []\n",
    "for gen in gen_tstats:\n",
    "    perm_max = cluster_test.max_tfr_cluster(next(gen),output='cluster_stat')\n",
    "    test_null.append(perm_max)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swb_ephys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
