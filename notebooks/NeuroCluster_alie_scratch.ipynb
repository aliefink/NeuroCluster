{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroCluster:\n",
    "<font size= 4> Non-parametric cluster-based permutation testing to identify neurophysiological encoding of continuous variables with time-frequency resolution\n",
    "\n",
    "Authors: Christina Maher & Alexandra Fink-Skular \\\n",
    "Updated: 06/24/2024 by AFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from glob import glob\n",
    "from scipy.stats import zscore, t, linregress, ttest_ind, ttest_rel, ttest_1samp \n",
    "import os \n",
    "import re\n",
    "import h5io\n",
    "import pickle \n",
    "import time \n",
    "import datetime \n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm \n",
    "from scipy.ndimage import label \n",
    "import statsmodels.formula.api as smf\n",
    "import tqdm\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# keep this so we can use our respective paths for testing\n",
    "current_user = 'alie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06252024\n"
     ]
    }
   ],
   "source": [
    "date = datetime.date.today().strftime('%m%d%Y')\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_user == 'christina':\n",
    "    base_dir = '/Users/christinamaher/Documents/GitHub/NeuroCluster/scripts/'\n",
    "    data_dir = '/Users/christinamaher/Documents/GitHub/NeuroCluster/'\n",
    "    tfr_dir  = f'{data_dir}tfr/'\n",
    "    anat_dir = f'{data_dir}anat/'\n",
    "elif current_user == 'alie':\n",
    "    # base_dir = '/hpc/users/finka03/NeuroCluster/NeuroCluster/'\n",
    "    # swb_dir  = '/sc/arion/projects/guLab/Alie/SWB/'\n",
    "    # tfr_dir  = f'{swb_dir}ephys_analysis/data/'\n",
    "    # beh_dir  = f'{swb_dir}swb_behav_models/data/behavior_preprocessed/'\n",
    "    # anat_dir = f'{swb_dir}ephys_analysis/recon_labels/'\n",
    "    # save_dir = f'{base_dir}/data/'\n",
    "    \n",
    "    base_dir = '/Users/alexandrafink/Documents/GraduateSchool/SaezLab/NeuroCluster/NeuroCluster/NeuroCluster/scripts/'\n",
    "    data_dir = '/Users/alexandrafink/Documents/GraduateSchool/SaezLab/SWB/'\n",
    "    tfr_dir  = f'{data_dir}ephys_analysis/data/'\n",
    "    beh_dir  = f'{data_dir}behavior_analysis/behavior_preprocessed/'\n",
    "    anat_dir = f'{data_dir}anat_recons/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load functions \n",
    "import sys\n",
    "sys.path.append(f'{base_dir}')\n",
    "# sys.path.append(f'{base_dir}scripts/')\n",
    "\n",
    "from tfr_cluster_test import *\n",
    "from helper_utils import *\n",
    "# from plotting_utils import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Format Input Data (Currently within-subject)\n",
    "- neural input: np.array (n_channels x n_epochs x n_freqs x n_times)\n",
    "- regressor data: np.array (numpy array: n_epochs x n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/alexandrafink/Documents/GraduateSchool/SaezLab/SWB/ephys_analysis/data/MS002/MS002_CpeOnset-tfr.h5 ...\n",
      "Adding metadata with 19 columns\n"
     ]
    }
   ],
   "source": [
    "# load epoched data for single subj\n",
    "if current_user == 'alie':\n",
    "    permute_var = 'decisionCPE'\n",
    "    subj_id     = 'MS002'   \n",
    "    power_epochs = mne.time_frequency.read_tfrs(fname=f'{tfr_dir}{subj_id}/{subj_id}_CpeOnset-tfr.h5')[0]\n",
    "elif current_user == 'christina':\n",
    "    permute_var = 'ev_zscore'\n",
    "    subj_id     = 'MS009'   \n",
    "    power_epochs = mne.time_frequency.read_tfrs(fname=f'{tfr_dir}/{subj_id}_tfr.h5')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ROI for single ROI anaylsis \n",
    "if current_user == 'alie':\n",
    "#     roi = 'ains'\n",
    "    # set all variables included mutliple regression \n",
    "    multi_reg_vars = ['GambleChoice','TrialEV','TotalProfit','decisionCPE']\n",
    "    # set main variable of interest for permutations \n",
    "    permute_var = 'decisionCPE'\n",
    "    # load subj behavior data \n",
    "#     beh_df = pd.read_csv(f'{beh_dir}{subj_id}_task_data')\n",
    "    beh_df = power_epochs.metadata.copy()\n",
    "    # beh_df['subj_id'] = subj_id\n",
    "    # add TrialEV to df\n",
    "    beh_df['TrialEV'] = beh_df.GambleEV - beh_df.SafeBet\n",
    "    # clean subj dataframe from fail trials/nan values in vars of interest     \n",
    "    # beh_df = beh_df[(beh_df.GambleChoice=='gamble')|(beh_df.GambleChoice=='safe')]\n",
    "#     beh_df = beh_df[(beh_df.Outcome=='good')|(beh_df.Outcome=='bad')]\n",
    "    \n",
    "    # zscore continuous variables \n",
    "    beh_df[multi_reg_vars[1:]] = pd.DataFrame({f'{var}':zscore(beh_df[var])  for var in multi_reg_vars[1:]})\n",
    "    # format final beh_df\n",
    "    beh_df = beh_df[multi_reg_vars].reset_index(drop=True) \n",
    "    # convert choice to categorical variable\n",
    "    beh_df['GambleChoice'] = beh_df['GambleChoice'].astype('category')\n",
    "\n",
    "elif current_user == 'christina':\n",
    "    beh_df = prepare_regressor_df(power_epochs)\n",
    "    ## new function for getting elecs in ROI\n",
    "    roi = ['lpfc','ofc']\n",
    "    roi_subj_elecs = prepare_anat_dic(roi, f'{anat_dir}master_labels.csv')\n",
    "    roi_subj_elecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### class TFR_Cluster_Test dev + debugging\n",
    "\n",
    "if current_user == 'alie':\n",
    "\n",
    "    # subset single electrode tfr data + behav data\n",
    "    dev_ch_idx = power_epochs.ch_names.index('laims2-laims3')\n",
    "    ch_name    = 'laims2-laims3'\n",
    "    tfr_data   = np.squeeze(power_epochs._data[:,dev_ch_idx,:,:].copy())\n",
    "    predictor_data = beh_df.copy()\n",
    "    \n",
    "    # predictor_data = predictor_data.drop(columns='subj_id')\n",
    "\n",
    "elif current_user == 'christina':\n",
    "    \n",
    "        # subset single electrode tfr data + behav data\n",
    "        # predictor_data = predictor_data.drop(columns=['condition','chosen_shape_current_trial','chosen_color_current_trial','chosen_shape_previous_trial','chosen_color_previous_trial','ev'])\n",
    "        tfr_data = np.squeeze(power_epochs._data[:,0,:,:].copy())\n",
    "        ch_name = power_epochs.info['ch_names'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subj_id', 'GambleChoice', 'TrialEV', 'TotalProfit', 'decisionCPE'], dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Find Real Clusters\n",
    "- Use TFRClusterTest class code to run multivariate regression\n",
    "- Allows for multiple regression implementation and pixel paralellization, so with more speed improvements will ultimately be worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tfr_cluster_test.TFR_Cluster_Test at 0x7f9cda4e7ac0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_test  = TFR_Cluster_Test(tfr_data,predictor_data,permute_var,ch_name,alternative='two-sided')\n",
    "cluster_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    4.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1072 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 3088 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 5680 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done 8848 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done 12592 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=-1)]: Done 16912 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=-1)]: Done 21808 tasks      | elapsed:   31.9s\n",
      "[Parallel(n_jobs=-1)]: Done 27280 tasks      | elapsed:   41.4s\n",
      "[Parallel(n_jobs=-1)]: Done 33328 tasks      | elapsed:   53.0s\n",
      "[Parallel(n_jobs=-1)]: Done 39952 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done 45030 out of 45030 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "betas, tstats = cluster_test.tfr_regression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cluster_data  = cluster_test.max_tfr_cluster(tstats,output='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    results = Parallel(n_jobs=n_jobs, verbose=5)(delayed(min_fn)(fit_fn, param_values, (subj_df), param_bounds) for param_values in param_combo_guesses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NeuroCluster single electrode workflow: \n",
    "\n",
    "# Step 1: Create TFR_Cluster_Test Object\n",
    "cluster_test  = TFR_Cluster_Test(tfr_data,predictor_data,permute_var,ch_name,alternative='two-sided')\n",
    "\n",
    "# Step 2: Run TFR regression to extract beta coefficients for predictor of interest (permute_var) & tstats for each pixel in TFR\n",
    "betas, tstats = cluster_test.tfr_regression()\n",
    "\n",
    "# Step 3: Find largest cluster(s) and return the max cluster statistic(s) and cluster's  frequencies x times indices\n",
    "max_cluster_data  = cluster_test.max_tfr_cluster(tstats,output='all')\n",
    "\n",
    "# Step 4: Create null distribution of maximum cluster statistics from permuted data\n",
    "null_cluster_distribution = cluster_test.compute_null_cluster_stats(max_cluster_data,num_permutations=10)\n",
    "\n",
    "# Step 5: Use null cluster statistic distribution from permutations to compute non-parametric p value \n",
    "cluster_pvalue = cluster_test.cluster_significance_test(null_cluster_distribution,alpha=0.05) #compute_cluster_pvalue cluster_significance_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFR_Cluster_Test(object):\n",
    "    \"\"\" \n",
    "    Single-electrode neurophysiology object class to identify time-frequency resolved neural activity correlates of complex behavioral variables using non-parametric \n",
    "    cluster-based permutation testing.   \n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    tfr_data       : (np.array) Single electrode tfr data matrix. Array of floats (n_epochs,n_freqs,n_times). \n",
    "    tfr_dims       : (tuple) Frequency and time dimensions of tfr_data. Tuple of integers (n_freq,n_times). \n",
    "    ch_name        : (str) Unique electrode identification label. String of characters.\n",
    "    predictor_data : (pd.DataFrame) Regressors from task behavior with continuous, discreet, or categorical data. DataFrame of (rows=n_epochs,columns=n_regressors). \n",
    "    permute_var    : (str) Column label for primary regressor of interest.\n",
    "      \n",
    "    Methods\n",
    "    ----------\n",
    "    **To-do: fill in methods info\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tfr_data, predictor_data, permute_var, ch_name, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - tfr_data       : (np.array) Single electrode tfr data matrix. Array of floats (n_epochs,n_freqs,n_times). \n",
    "        - predictor_data : (pd.DataFrame) Task-based regressor data with dtypes continuous/discreet(int64/float) or categorical(pd.Categorical). DataFrame of (n_epochs,n_regressors).\n",
    "        - permute_var    : (str) Column label for primary regressor of interest. Array of 1d integers or floats (n_epochs,).\n",
    "        - ch_name        : (str) Unique electrode identification label. String of characters.  \n",
    "        - **kwargs       : (optional) alternative, alpha, cluster_shape\n",
    "        \"\"\"\n",
    "\n",
    "        self.tfr_data       = tfr_data  # single electrode tfr data\n",
    "        self.predictor_data = predictor_data # single subject behav data\n",
    "        self.tfr_dims       = self.tfr_data.shape[1:] # time-frequency dims of electrode data (n_freqs x n_times)\n",
    "        self.permute_var    = permute_var # variable to permute in regression model \n",
    "        self.ch_name        = ch_name # channel name for single electrode tfr data\n",
    "\n",
    "    def tfr_regression(self):\n",
    "        \"\"\"\n",
    "        Performs univariate or multivariate OLS regression across tfr matrix for all pixel-level time-frequency power data and task-based predictor variables. Regressions are parallelized across pixels.\n",
    "\n",
    "        Returns:\n",
    "        - tfr_betas  : (np.array) Matrix of beta coefficients for predictor of interest for each pixel regression. Array of (n_freqs,n_times). \n",
    "        - tfr_tstats : (np.array) Matrix of t-statistics from coefficient estimates for predictor of interest for each pixel regression. Array of (n_freqs,n_times). \n",
    "        \"\"\"\n",
    "        \n",
    "        # Prepare arguments for parallelization`using tfr matrix indices converted to list of tuples (freq x power)\n",
    "        pixel_args = [self.make_pixel_df(self.tfr_data[:,freq_idx,time_idx]) for freq_idx,time_idx in self.expand_tfr_indices()]\n",
    "        \n",
    "        # run pixel permutations in parallel \n",
    "        expanded_results = Parallel(n_jobs=-1, verbose=5)(\n",
    "                        delayed(self.pixel_regression)(args)\n",
    "                            for args in pixel_args)      \n",
    "        \n",
    "        # preallocate np arrays for betas + tstats\n",
    "        tfr_betas  = np.zeros((self.tfr_dims))\n",
    "        tfr_tstats = np.zeros((self.tfr_dims))\n",
    "\n",
    "        # expanded_results is a list of tuples (beta,tstat) for every pixel \n",
    "        for count,(freq_idx,time_idx) in enumerate(self.expand_tfr_indices()):\n",
    "            tfr_betas[freq_idx,time_idx]  = expanded_results[count][0]\n",
    "            tfr_tstats[freq_idx,time_idx] = expanded_results[count][1]\n",
    "        \n",
    "        return tfr_betas, tfr_tstats\n",
    "\n",
    "    def pixel_regression(self,pixel_df):\n",
    "        \"\"\"\n",
    "        Fit pixel-wise univariate or multivariate OLS regression model and extract beta coefficient and t-statistic for predictor of interest (self.permute_var). \n",
    "\n",
    "        Args:\n",
    "        - pixel_df   : (pd.DataFrame) Pixel-level regression dataframe with power epochs data and behavioral regressors. DataFrame of (n_epochs, n_regressors+1). \n",
    "                                      Regressor column data must be continuous(dtype=float), discrete(dtype=int), or categorical(dtype=pd.Categorical). \n",
    "        \n",
    "        Returns:\n",
    "        - pixel_beta : (np.array) Beta coefficient for predictor of interest from pixel-wise regression. Array of 1d float (1,)\n",
    "        - pixel_tval : (np.array) Observed t-statistic for predictor of interest from pixel-wise regression. Array of 1d float (1,)\n",
    "        \"\"\"\n",
    "\n",
    "        # formula should be in form 'col_name + col_name' if col is categorical then should be 'C(col_name)'  \n",
    "        formula    = 'pow ~ 1 + ' + (' + ').join(['C('+col+')' if pd.api.types.is_categorical_dtype(pixel_df[col])\n",
    "                                    else col for col in pixel_df.columns[~pixel_df.columns.isin(['pow'])].tolist()])\n",
    "        \n",
    "        pixel_model = smf.ols(formula,pixel_df,missing='drop').fit()\n",
    "\n",
    "        return (pixel_model.params[self.permute_var],pixel_model.tvalues[self.permute_var])\n",
    "\n",
    "    def max_tfr_cluster(self,tfr_tstats,alternative='two-sided',output='all',clust_struct=np.ones(shape=(3,3))):\n",
    "\n",
    "        \"\"\"\n",
    "        Identify time-frequency clusters of neural activity that are significantly correlated with the predictor of interest (self.permute_var). Clusters are identified \n",
    "        from neighboring pixel regression t-statistics for the predictor of interest that exceed the tcritical threshold from the alternate hypothesis. \n",
    "\n",
    "        Args:\n",
    "        - tfr_tstats       : (np.array) Pixel regression tstatistic from coefficient estimates for predictor of interest. Array of floats (n_freqs,n_times). \n",
    "        - alternative      : (str) Alternate hypothesis for t-test. Must be 'two-sided','greater', or 'less'. Default is 'two-sided'. \n",
    "        - output           : (str) Output format for max cluster statistics. Must be 'all', 'cluster_stat', or 'freq_time'. Default is 'all'.\n",
    "        - clust_struct     : (np.array) Binary matrix to specify cluster structure for scipy.ndimage.label. Array of (3,3). \n",
    "                                        Default is np.ones.shape(3,3), to allow diagonal cluster pixels (Not the scipy.ndimage.label default).\n",
    "                                        https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.label.html\n",
    "\n",
    "        Returns:\n",
    "        - max_cluster_data : (list) Beta coefficient for predictor of interest for each pixel regression. List (len=2 if 'two-sided') of dict(s).\n",
    "                                    If output = 'all', return dictionary of maximum cluster statistic ('cluster_stat' : sum of pixel t-statistics), \n",
    "                                    cluster frequency indices ('freq_idx':(freq_x,freq_y)), and cluster time indices ('time_idx':(time_x,time_y)). \n",
    "                                    If output = 'cluster_stat', return only [{cluster_stat}]. If output = 'freq_time', return only {freq_idx,time_idx}\n",
    "                                    ** If no clusters are found, max_cluster_data = {[]}\n",
    "        \"\"\"\n",
    "        \n",
    "        max_cluster_data = []\n",
    "        # Create binary matrix from tfr_tstats by thresholding pixel t-statistics by tcritical. (1 = pixel t-statistic exceeded tcritical threshold)\n",
    "        for binary_mat in self.threshold_tfr_tstat(tfr_tstats,alternative):\n",
    "            # test whether there are any pixels above tcritical threshold\n",
    "            if np.sum(binary_mat) != 0: \n",
    "                # Find clusters of pixels with t-statistics exceeding tcritical\n",
    "                cluster_label, num_clusters = label(binary_mat,clust_struct)\n",
    "                # use argmax to find index of largest absolute value of cluster t statistic sums \n",
    "                max_label = np.argmax([np.abs(np.sum(tfr_tstats[cluster_label==i+1])) for i in range(num_clusters)])\n",
    "                # use max_label index to compute cluster tstat sum (without absolute value)\n",
    "                max_clust_stat = np.sum(tfr_tstats[cluster_label==max_label+1])\n",
    "                # find \n",
    "                clust_freqs, clust_times = [(np.min(arr),np.max(arr)) for arr in np.where(cluster_label == max_label)]\n",
    "\n",
    "                if output == 'all':\n",
    "                    max_cluster_data.append({'cluster_stat':max_clust_stat,'freq_idx':clust_freqs,'time_idx':clust_times})\n",
    "                elif output == 'cluster_stat':\n",
    "                    max_cluster_data.append({'cluster_stat':max_clust_stat})\n",
    "                elif output == 'freq_time':\n",
    "                    max_cluster_data.append({'freq_idx':clust_freqs,'time_idx':clust_times})\n",
    "\n",
    "            else: # if there is no cluster, keep max_cluster_data empty list\n",
    "                continue\n",
    "\n",
    "        return max_cluster_data\n",
    "\n",
    "    def compute_tcritical(self,alternative ='two-sided',alpha=0.05):\n",
    "        \"\"\"\n",
    "        Calculate critical t-values for regression model.\n",
    "        \n",
    "        Args:\n",
    "        - alternative : (str) Alternate hypothesis for t-test. Must be 'two-sided','greater', or 'less'. Default is 'two-sided'.\n",
    "        - alpha       : (float) Significance level. Default is 0.05.\n",
    "\n",
    "        Returns:\n",
    "        - tcritical   : (float) Critical t-statistic for hypothesis test. Positive value when alternative = 'two-sided' or 'greater'. Negative when alternative = 'less'. \n",
    "        \"\"\"\n",
    "\n",
    "        # Set number of tails for t-tests using 'alternative' parameter input string. \n",
    "            # tails = 2 if alternative = 'two-sided' (two tailed hypothesis test)\n",
    "            # tails = 1 if alternative = 'greater' or 'less' (one tailed hypothesis test)\n",
    "        tails = len(alternative.split('-')) \n",
    "\n",
    "        # Calculate degrees of freedom (N-k-1) \n",
    "        deg_free = float(len(self.predictor_data)-len(self.predictor_data.columns)-1) #### predictor data must only include regressors in columns\n",
    "\n",
    "        # Return tcritical from t-distribution. Significance level is alpha/2 for two tailed hypothesis tests (alternative = 'two-sided').\n",
    "        return (t.ppf(1-(alpha/tails),deg_free) if alternative != 'less' else np.negative(t.ppf(1-(alpha/tails),deg_free)))\n",
    "\n",
    "    def threshold_tfr_tstat(self,tfr_tstats,alternative='two-sided'):\n",
    "        \"\"\"\n",
    "        Threshold tfr t-statistic matrix using tcritical.\n",
    "\n",
    "        Args:\n",
    "        - tfr_tstats  : (np.array) Matrix of t-statistics from pixel-wise regressions. Array of floats (n_freqs, n_times). \n",
    "        - alternative : (str) Type of hypothesis test for t-distribution. Must be 'two-sided', 'greater', 'less'. Default is 'two-sided'.\n",
    "\n",
    "        Returns:\n",
    "        - binary_mat  : (np.array) Binary matrix results of pixel-wise t-tests. Pixel = 1 when tstatistic > tcritical, else pixel = 0. List of array(s) (n_freqs, n_times).\n",
    "        \"\"\"\n",
    "\n",
    "        if alternative == 'two-sided': \n",
    "            return [(tfr_tstats>self.compute_tcritical()).astype(int), (tfr_tstats<np.negative(self.compute_tcritical()).astype(int))]\n",
    "\n",
    "        elif alternative == 'greater':\n",
    "            return [(tfr_tstats>self.compute_tcritical(tails=1,alternative='greater')).astype(int)]\n",
    "\n",
    "        elif alternative == 'less':\n",
    "            return [(tfr_tstats<self.compute_tcritical(tails=1,alternative='less')).astype(int)] \n",
    "        else: \n",
    "            raise ValueError('Alternative hypothesis must be two-sided, greater, or less not {alternative}')\n",
    "    \n",
    "    def expand_tfr_indices(self):\n",
    "        \"\"\"\n",
    "        Create list of tfr pixel indices for parallelized tfr_regression.\n",
    "\n",
    "        Returns:\n",
    "        - iter_tup : (list) Time-frequency indices for all pixels in tfr_data. List of tuples [(freq_x_index,freq_y_index),(time_x_index,time_y_index)]        \n",
    "        \"\"\"\n",
    "\n",
    "        return list(map(tuple,np.unravel_index(np.dstack(([*np.indices(self.tfr_dims)])),np.product(self.tfr_dims)\n",
    "                            )[0].reshape(np.product(np.dstack(([*np.indices(self.tfr_dims)])).shape[:2]),-1)))\n",
    "\n",
    "    def make_pixel_df(self,epoch_data):\n",
    "        \"\"\"\n",
    "        Format input data for pixel regression.  input data. Make pixel-level (frequency x timepoint) dataframe. Add tfr power data for single pixel to predictor_df. \n",
    "\n",
    "        Args:\n",
    "        - epoch_data : (str) Alternate hypothesis for t-test. Must be 'two-sided','greater', or'less'. Default is 'two-sided'. Array of 1d integers or floats (n_epochs,).\n",
    "        \n",
    "        Returns:\n",
    "        - pixel_df   : (pd.DataFrame) Pixel regression input dataframe containing power epochs and task-based behavioral regressor data (dtype=int/float/pd.Categorical). \n",
    "                                      DataFrame of (n_epochs, n_regressors+1). \n",
    "        \"\"\"\n",
    "        return self.predictor_data.assign(pow=epoch_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### start permutation functions here\n",
    "\n",
    "    # def permuted_tfr_regression(self):\n",
    "    #     \"\"\"\n",
    "    #     Run permuted tfr regression \n",
    "\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     iter_tup = self.expand_tfr_indices()\n",
    "\n",
    "    #     # either precompute pixel_args before passing to parallel, or run all together in loop. - check later!! \n",
    "    #     perm_args = [self.make_pixel_df(self.tfr_data[:,freq_idx,time_idx]) for freq_idx,time_idx in iter_tup]\n",
    "        \n",
    "    #     # run pixel permutations in parallel \n",
    "    #     permuted_results = Parallel(n_jobs=-1, verbose=5)(\n",
    "    #                     delayed(self.pixel_regression)(args)\n",
    "    #                         for args in perm_args)      \n",
    "        \n",
    "    #     # preallocate np arrays for betas + tstats\n",
    "    #     tfr_betas = np.zeros((self.tfr_dims))\n",
    "    #     tfr_tstats = np.zeros((self.tfr_dims))\n",
    "\n",
    "    #     # expanded_results is a list of tuples (beta,tstat) for every pixel \n",
    "    #     for count,(freq_idx,time_idx) in enumerate(iter_tup):\n",
    "    #         tfr_betas[freq_idx,time_idx] = permuted_results[count][0]\n",
    "    #         tfr_tstats[freq_idx,time_idx] = permuted_results[count][1]\n",
    "        \n",
    "    #     # return tfr_betas, tfr_tstats\n",
    "        \n",
    "\n",
    "    #     # permute predictor data\n",
    "    #     self.predictor_data = self.permute_predictor() # Permute predictor variable\n",
    "        \n",
    "    #     # Run regression on permuted data + extract tstats only\n",
    "    #     perm_tstats = self.tfr_regression(permutation=True)  \n",
    "    #     # extract cluster statistics for permutation\n",
    "    #     perm_cluster_stat = self.max_tfr_cluster(perm_tstats, output='cluster_stat')  # Get cluster statistics\n",
    "    #     del perm_tstats  # Delete objects to free up memory\n",
    "    #     # check if any clusters detected in permuted tfr regression \n",
    "    #     if len(perm_cluster_stat) !=0 :\n",
    "    #         return perm_cluster_stat\n",
    "    #     else: # return list of empty cluster stats with length of alternative hypothesis ('two_sided'=length 2, less or greater = length 1)\n",
    "    #         return [{'cluster_stat':0}]*len(alternative.split('_'))\n",
    "\n",
    "    # def permuted_tfr_regression(self):\n",
    "    #     \"\"\"\n",
    "    #     Run permuted tfr regression \n",
    "    #     \"\"\"\n",
    "\n",
    "    #     # permute predictor data\n",
    "    #     self.predictor_data = self.permute_predictor() # Permute predictor variable\n",
    "\n",
    "    #     # Run regression on permuted data + extract tstats only\n",
    "    #     perm_tstats = self.tfr_regression(permutation=True)  \n",
    "    #     # extract cluster statistics for permutation\n",
    "    #     perm_cluster_stat = self.max_tfr_cluster(perm_tstats, output='cluster_stat')  # Get cluster statistics\n",
    "    #     del perm_tstats  # Delete objects to free up memory\n",
    "    #     # check if any clusters detected in permuted tfr regression \n",
    "    #     if len(perm_cluster_stat) !=0 :\n",
    "    #         return perm_cluster_stat\n",
    "    #     else: # return list of empty cluster stats with length of alternative hypothesis ('two_sided'=length 2, less or greater = length 1)\n",
    "    #         return [{'cluster_stat':0}]*len(alternative.split('_'))\n",
    "\n",
    "\n",
    "\n",
    "    # def permute_predictor(self):\n",
    "    #     \"\"\"\n",
    "    #     Permute predictor variable of interest for permutation testing.\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     permuted_predictor_data = self.predictor_data.copy()\n",
    "    #     permuted_predictor_data[self.permute_var] = np.random.permutation(permuted_predictor_data[self.permute_var].values)\n",
    "\n",
    "    #     return permuted_predictor_data\n",
    "    \n",
    "\n",
    "    # def cluster_significance_test(self, null_distribution,alpha=0.05,alternative='two-sided'):\n",
    "    #     \"\"\"\n",
    "    #     Compute non-parametric pvalue from cluster permutation data \n",
    "\n",
    "    #             - alpha (float): Significance level. Default is 0.05.\n",
    "\n",
    "    #     \"\"\"\n",
    "        \n",
    "    #     return cluster_pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current to-dos: \n",
    "\n",
    "- reformat max_cluster_data freq/time indices returns\n",
    "- permutation class functions \n",
    "- speed:\n",
    "    https://joblib.readthedocs.io/en/latest/parallel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from joblib import Parallel, delayed, parallel_config\n",
    "\n",
    "# with parallel_config(backend=\"loky\", inner_max_num_threads=2):\n",
    "#     results = Parallel(n_jobs=4)(delayed(func)(x, y) for x, y in data)\n",
    "\n",
    "# with parallel_config(backend='custom', endpoint='http://compute',\n",
    "#                      api_key='42'):\n",
    "#     Parallel()(delayed(some_function)(i) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# def parallel_apply_along_axis(func1d, axis, arr, *args, **kwargs):\n",
    "#     \"\"\"\n",
    "#     Like numpy.apply_along_axis(), but takes advantage of multiple\n",
    "#     cores.\n",
    "#     \"\"\"        \n",
    "#     # Effective axis where apply_along_axis() will be applied by each\n",
    "#     # worker (any non-zero axis number would work, so as to allow the use\n",
    "#     # of `np.array_split()`, which is only done on axis 0):\n",
    "#     effective_axis = 1 if axis == 0 else axis\n",
    "#     if effective_axis != axis:\n",
    "#         arr = arr.swapaxes(axis, effective_axis)\n",
    "\n",
    "#     # Chunks for the mapping (only a few chunks):\n",
    "#     chunks = [(func1d, effective_axis, sub_arr, args, kwargs)\n",
    "#               for sub_arr in np.array_split(arr, multiprocessing.cpu_count())]\n",
    "\n",
    "#     pool = multiprocessing.Pool()\n",
    "#     individual_results = pool.map(unpacking_apply_along_axis, chunks)\n",
    "#     # Freeing the workers:\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "\n",
    "#     return np.concatenate(individual_results)\n",
    "\n",
    "# def unpacking_apply_along_axis(all_args):\n",
    "#     \"\"\"…\"\"\"\n",
    "#     (func1d, axis, arr, args, kwargs) = all_args\n",
    "#     …\n",
    "\n",
    "# https://stackoverflow.com/questions/45526700/easy-parallelization-of-numpy-apply-along-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/76339092/convert-joblib-parallel-to-multiprocessing-pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "# import numpy as np\n",
    "\n",
    "# def my_function(x):\n",
    "#     pass     # do something and return something\n",
    "\n",
    "# if __name__ == '__main__':    \n",
    "#     X = np.arange(6).reshape((3,2))\n",
    "#     pool = Pool(processes = 4)\n",
    "#     results = pool.map(my_function, map(lambda x: x, X))\n",
    "#     pool.close()\n",
    "#     pool.join()\n",
    "\n",
    "# https://stackoverflow.com/questions/16468717/iterating-over-numpy-matrix-rows-to-apply-a-function-each\n",
    "\n",
    "# https://stackoverflow.com/questions/8079061/function-application-over-numpys-matrix-row-column\n",
    "\n",
    "# https://docs.python.org/3/library/itertools.html#itertools.filterfalse\n",
    "\n",
    "# https://medium.com/pythoneers/vectorization-in-python-an-alternative-to-python-loops-2728d6d7cd3e\n",
    "\n",
    "# https://realpython.com/python-map-function/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/pythoneers/vectorization-in-python-an-alternative-to-python-loops-2728d6d7cd3e\n",
    "# https://realpython.com/python-map-function/\n",
    "\n",
    "# https://joblib.readthedocs.io/en/latest/auto_examples/parallel_memmap.html#sphx-glr-auto-examples-parallel-memmap-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://medium.com/@nirmalya.ghosh/13-ways-to-speedup-python-loops-e3ee56cd6b73\n",
    "\n",
    "# def test_08_v1(n):\n",
    "#   # Improved version\n",
    "#   # (Efficiently calculates the nth Fibonacci\n",
    "#   # number using a generator)\n",
    "#   a, b = 0, 1\n",
    "#   for _ in range(n):\n",
    "#     yield a\n",
    "#     a, b = b, a + b\n",
    "\n",
    "\n",
    "#     def some_function_X(x):\n",
    "#   # This would normally be a function containing application logic\n",
    "#   # which required it to be made into a separate function\n",
    "#   # (for the purpose of this test, just calculate and return the square)\n",
    "#   return x**2\n",
    "\n",
    "# def test_09_v0(numbers):\n",
    "#   # Baseline version (Inefficient way)\n",
    "#   output = []\n",
    "#   for i in numbers:\n",
    "#     output.append(some_function_X(i))\n",
    "\n",
    "#   return output\n",
    "\n",
    "# def test_09_v1(numbers):\n",
    "#   # Improved version\n",
    "#   # (Using Python's built-in map() function)\n",
    "#   output = map(some_function_X, numbers)\n",
    "#   return output\n",
    "\n",
    "\n",
    "# def test_12_v0(numbers):\n",
    "#   # Baseline version (Inefficient way)\n",
    "#   filtered_data = []\n",
    "#   for i in numbers:\n",
    "#     filtered_data.extend(list(\n",
    "#         filter(lambda x: x % 5 == 0,\n",
    "#                 range(1, i**2))))\n",
    "  \n",
    "#   return filtered_data\n",
    "\n",
    "# from itertools import filterfalse\n",
    "\n",
    "\n",
    "\n",
    "# def test_12_v1(numbers):\n",
    "#   # Improved version\n",
    "#   # (using filterfalse)\n",
    "#   filtered_data = []\n",
    "#   for i in numbers:\n",
    "#     filtered_data.extend(list(\n",
    "#         filterfalse(lambda x: x % 5 != 0,\n",
    "#                     range(1, i**2))))\n",
    "    \n",
    "#     return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel(n_jobs=2, prefer=\"threads\")(\n",
    "#     delayed(sqrt)(i ** 2) for i in range(10))\n",
    "\n",
    "\n",
    "# from joblib import parallel_config\n",
    "# with parallel_config(backend='threading', n_jobs=2):\n",
    "#    Parallel()(delayed(sqrt)(i ** 2) for i in range(10))\n",
    "\n",
    "#    shared_set = set()\n",
    "# def collect(x):\n",
    "#    shared_set.add(x)\n",
    "\n",
    "# Parallel(n_jobs=2, require='sharedmem')(\n",
    "#     delayed(collect)(i) for i in range(5))\n",
    "# [None, None, None, None, None]\n",
    "\n",
    "\n",
    "# with Parallel(n_jobs=2) as parallel:\n",
    "#    accumulator = 0.\n",
    "#    n_iter = 0\n",
    "#    while accumulator < 1000:\n",
    "#        results = parallel(delayed(sqrt)(accumulator + i ** 2)\n",
    "#                           for i in range(5))\n",
    "#        accumulator += sum(results)  # synchronization barrier\n",
    "#        n_iter += 1\n",
    "\n",
    "# (accumulator, n_iter) \n",
    "# # https://joblib.readthedocs.io/en/latest/parallel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import Pool\n",
    "# from multiprocessing import Pool\n",
    "# #Define a worker — a function which will be executed in parallel\n",
    "# def worker(x):\n",
    "#  return x*x\n",
    "# #Assuming you want to use 3 processors\n",
    "# num_processors = 3\n",
    "# #Create a pool of processors\n",
    "# p=Pool(processes = num_processors)\n",
    "# #get them to work in parallel\n",
    "# output = p.map(worker,[i for i in range(0,3)])\n",
    "# print(output)\n",
    "\n",
    "# from multiprocessing import Pool\n",
    "# import workers\n",
    "# if __name__ ==  '__main__': \n",
    "#  num_processors = 3\n",
    "#  p=Pool(processes = num_processors)\n",
    "#  output = p.map(workers.worker,[i for i in range(0,3)])\n",
    "#  print(output)\n",
    "# https://medium.com/@grvsinghal/speed-up-your-code-using-multiprocessing-in-python-36e4e703213e\n",
    "# https://medium.com/@grvsinghal/speed-up-your-python-code-using-multiprocessing-on-windows-and-jupyter-or-ipython-2714b49d6fac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/42220458/what-does-the-delayed-function-do-when-used-with-joblib-in-python\n",
    "\n",
    "# import joblib\n",
    "\n",
    "# @joblib.delayed\n",
    "# def getHog(image):\n",
    "#     \"\"\"Some time-consuming function on an image\"\"\"\n",
    "#     ...\n",
    "\n",
    "# # Running this in parallel\n",
    "# with joblib.Parallel(backend=\"loky\", n_jobs=8) as parallel:\n",
    "#     result = parallel(getHog(img) for img in allImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def my_function(x):\n",
    "#     \"\"\"The function you want to compute in parallel.\"\"\"\n",
    "#     x += 1\n",
    "#     return x\n",
    "\n",
    "# import multiprocessing\n",
    "# import workers\n",
    "\n",
    "# pool = multiprocessing.Pool()\n",
    "# results = pool.map(workers.my_function, [1,2,3,4,5,6])\n",
    "# # print(results)\n",
    "# # \n",
    "# # https://stackoverflow.com/questions/23641475/multiprocessing-working-in-python-but-not-in-ipython/23641560#23641560\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "power_epochs.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = [(np.squeeze(power_epochs._data[:,ch_ix,:,:].copy()),\n",
    "#                             predictor_data,permute_var,ch_name) for ch_ix, ch_name\n",
    "#                             in enumerate(power_epochs.ch_names)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://joblib.readthedocs.io/en/latest/parallel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST PERMUTATIONS \n",
    "start = time.time() # start timer\n",
    "\n",
    "\n",
    "args = [(np.squeeze(power_epochs._data[:,ch_ix,:,:].copy()),\n",
    "         predictor_data,permute_var,ch_name) for ch_ix, ch_name\n",
    "         in enumerate(power_epochs.ch_names)]\n",
    "\n",
    "# Perform permutations in parallel\n",
    "elec_Cluster_objs = Parallel(n_jobs=-1, verbose=5)(\n",
    "delayed(TFR_Cluster_Test)(*a)\n",
    "for a in args)\n",
    "\n",
    "#         pickle.dump(elec_permuted_data, open(f'{results_dir}{subj_id}_{c}_perm_clusters{date}.pkl', \"wb\")) \n",
    "\n",
    "end = time.time()    \n",
    "print('{:.4f} s'.format(end-start)) # print time elapsed for computation (approx 4 seconds per permutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time() # start timer\n",
    "\n",
    "## run simple linear regression on electrodes in parallel to speed up computation - I did this for just the subset OFC electrodes.\n",
    "subj_all_elec_data = Parallel(n_jobs=-1,verbose=5)(\n",
    "    delayed(elec_obj.tfr_cluster_results())(num_permutations=None\n",
    "        ) for elec_obj in elec_Cluster_objs)\n",
    "\n",
    "end = time.time()    \n",
    "print('{:.4f} s'.format(end-start)) # print time elapsed for computation (approx 20 seconds per channel)\n",
    "\n",
    "# save subj cluster data for all electrodes\n",
    "# pickle.dump(subj_all_elec_data, open(f'{save_dir}{subj_id}_all_elec_real_clusters.pkl', \"wb\")) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time() # start timer\n",
    "\n",
    "# ## run simple linear regression on electrodes in parallel to speed up computation - I did this for just the subset OFC electrodes.\n",
    "# subj_all_elec_data = Parallel(n_jobs=-1,verbose=5)(\n",
    "#     delayed(TFR_Cluster_Test.tfr_cluster_results())(\n",
    "#         np.squeeze(power_epochs._data[:,ch_ix,:,:].copy()),predictor_data,permute_var,ch_name\n",
    "#         ) for ch_ix, ch_name in enumerate(power_epochs.ch_names))\n",
    "\n",
    "# end = time.time()    \n",
    "# print('{:.4f} s'.format(end-start)) # print time elapsed for computation (approx 20 seconds per channel)\n",
    "\n",
    "# # save subj cluster data for all electrodes\n",
    "# pickle.dump(subj_all_elec_data, open(f'{save_dir}{subj_id}_all_elec_real_clusters.pkl', \"wb\")) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_test  = TFR_Cluster_Test(tfr_data,beh_df,permute_var,'laims2-laims3')\n",
    "betas, tstats = cluster_test.tfr_regression()\n",
    "cluster_data  = cluster_test.max_tfr_cluster(tstats)\n",
    "\n",
    "cluster_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perm = TFR_Cluster_Test(tfr_data,beh_df,permute_var,'laims2-laims3').tfr_cluster_results(num_permutations=10)\n",
    "test_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfr_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3/tutorial/classes.html#generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Extract Surrogate Clusters from Pixel-wise Permutation\n",
    "- For loop for each electrode- \n",
    "- Run each permutation (1000x) in parallel within electrode loop\n",
    "- Calculate max cluster p value for each +/- cluster for each electrode\n",
    "- Save permuted cluster statistics for each electrode \n",
    "\n",
    "DEPENDENCIES: permuted_tfr_cluster_test, tfr_cluster_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perm = TFR_Cluster_Test(tfr_data,beh_df,permute_var,'laims2-laims3').tfr_cluster_results(num_permutations=10)\n",
    "test_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_cluster_results = TFR_Cluster_Test(tfr_data,beh_df,permute_var,'laims2-laims3').tfr_cluster_results(num_permutations=200)\n",
    "perm_cluster_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_cluster_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_full_perm = TFR_Cluster_Test(tfr_data,beh_df,permute_var,ch_name).tfr_cluster_results(num_permutations=200)\n",
    "# test_full_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TFR_Cluster_Test(object):\n",
    "    \"\"\"Class for time-frequency resolved cluster permutation testing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tfr_data : array, float, shape(freqs,times,n_epochs)\n",
    "        Single electrode tfr data \n",
    "    predictor_data : DataFrame, shape(n_epochs,n_regressors)\n",
    "        Single subject behav data \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tfr_data, predictor_data, permute_var, ch_name, **kwargs):\n",
    "\n",
    "        \"\"\"Constructor for the Environment class\n",
    "        This function runs every time we create an instance of the class Environment\n",
    "        To learn more about how constructors work: https://www.udacity.com/blog/2021/11/__init__-in-python-an-overview.html\"\"\"\n",
    "\n",
    "        # \"self\" is just a convention that binds the attributes and methods of a class with the arguments of a given instance\n",
    "\n",
    "        self.tfr_data       = tfr_data  # single electrode tfr data\n",
    "        self.predictor_data = predictor_data # single subject behav data\n",
    "        self.tfr_dims       = self.tfr_data.shape[1:] # dims of single electrode tfr data (n_freqs x n_times)\n",
    "        self.permute_var    = permute_var # variable to permute in regression model\n",
    "        self.ch_name        = ch_name # channel name for single electrode tfr data\n",
    "\n",
    "    def tfr_cluster_results(self,num_permutations=1000):\n",
    "\n",
    "        ### run tfr regression + get tstats/betas\n",
    "        tfr_betas, tfr_tstats = self.tfr_regression()\n",
    "        max_cluster_data      = self.max_tfr_cluster(tfr_tstats)\n",
    "\n",
    "        if len(max_cluster_data) == 0:\n",
    "            print(f'{self.ch_name} has no clusters')\n",
    "\n",
    "        else: \n",
    "            if not num_permutations: # if num_permutations == None (do not run permutations)     \n",
    "                return max_cluster_data\n",
    "            else:\n",
    "                print(f'starting tfr cluster permutation tests for {self.ch_name}')\n",
    "\n",
    "                perm_start = time.time()\n",
    "                # run permutations - run regression, get max data \n",
    "                permutation_cluster_stats = Parallel(n_jobs=-1, verbose=12)(\n",
    "                    delayed(self.permuted_tfr_regression)(cluster_test) for _ in range(num_permutations))\n",
    "\n",
    "                print(f'{self.ch_name} permutation time: ', '{:.2f}'.format(time.time()-perm_start))\n",
    "\n",
    "                # compute pvalue for real tfr clusters in max_cluster_data from permutation stats \n",
    "                for ix,cluster in enumerate(max_cluster_data): \n",
    "                    cluster['elec_id'] = self.ch_name\n",
    "                    perm_counter = 0\n",
    "                    for perm_result in permutation_cluster_stats:\n",
    "                        if np.abs(perm_result[ix]['cluster_stat']) > np.abs(cluster['cluster_stat']):\n",
    "                            perm_counter += 1\n",
    "                    if perm_counter != 0: \n",
    "                        cluster['pvalue'] = perm_counter/num_permutations \n",
    "                    else:\n",
    "                        cluster['pvalue'] = 1/num_permutations # minimum possible p value for number of permutations    \n",
    "                        max_cluster_data[ix] = cluster\n",
    "\n",
    "                return max_cluster_data   \n",
    "\n",
    "    def tfr_regression(self,permutation=False):\n",
    "\n",
    "        iter_tup = self.expand_tfr_indices()\n",
    "\n",
    "        # Prepare arguments for the permutation function`\n",
    "        start = time.time()\n",
    "\n",
    "        # either precompute pixel_args before passing to parallel, or run all together in loop. - check later!!\n",
    "        pixel_args = [self.make_pixel_df(self.tfr_data[:,freq_idx,time_idx]) for freq_idx,time_idx in iter_tup]\n",
    "\n",
    "        # run pixel permutations in parallel \n",
    "        expanded_results = Parallel(n_jobs=-1, verbose=12)(\n",
    "                            delayed(self.pixel_regression)(args)\n",
    "                            for args in pixel_args)      \n",
    "        if not permutation:\n",
    "            # preallocate np arrays for betas + tstats\n",
    "            tfr_betas = np.zeros((self.tfr_dims))\n",
    "            tfr_tstats = np.zeros((self.tfr_dims))\n",
    "\n",
    "            # expanded_results is a list of tuples (beta,tstat) for every pixel \n",
    "            for count,(freq_idx,time_idx) in enumerate(iter_tup):\n",
    "                tfr_betas[freq_idx,time_idx] = expanded_results[count][0]\n",
    "                tfr_tstats[freq_idx,time_idx] = expanded_results[count][1]\n",
    "\n",
    "            print(f'tfr regression time: ', '{:.2f}'.format(time.time()-start))\n",
    "\n",
    "            return tfr_betas, tfr_tstats\n",
    "\n",
    "        else:\n",
    "            # preallocate np arrays for tstats\n",
    "            perm_tstats = np.zeros((self.tfr_dims))\n",
    "\n",
    "            # expanded_results is a list of tuples (betas,tstat) for every pixel \n",
    "            for count,(freq_idx,time_idx) in enumerate(iter_tup):\n",
    "                perm_tstats[freq_idx,time_idx] = expanded_results[count][1]\n",
    "\n",
    "            # return only tstats for permutation regressions \n",
    "            return perm_tstats\n",
    "\n",
    "    def max_tfr_cluster(self,tfr_tstats,alternative='two-sided',clust_struct=np.ones(shape=(3,3)),output='all'):\n",
    "        \n",
    "        max_cluster_data = []\n",
    "\n",
    "        for binary_mat in self.threshold_tfr_tstat(tfr_tstats,alternative = alternative):\n",
    "            if np.sum(binary_mat) != 0: # test whether there are any pixels > t critical\n",
    "                cluster_label, num_clusters = label(binary_mat,clust_struct)\n",
    "                # use argmax to find index of largest absolute value of cluster t statistic sums \n",
    "                max_label = np.argmax([np.abs(np.sum(tfr_tstats[cluster_label==i+1])) for i in range(num_clusters)])\n",
    "                # use max_label index to compute cluster tstat sum (without absolute value)\n",
    "                max_clust_stat = np.sum(tfr_tstats[cluster_label==max_label+1])\n",
    "                clust_freqs, clust_times = [(np.min(arr),np.max(arr)) for arr in np.where(cluster_label == max_label)]\n",
    "\n",
    "                if output == 'all':\n",
    "                    max_cluster_data.append({'cluster_stat':max_clust_stat,'freq_idx':clust_freqs,'time_idx':clust_times})\n",
    "                elif output == 'cluster_stat':\n",
    "                    max_cluster_data.append({'cluster_stat':max_clust_stat})\n",
    "                elif output == 'freq_time':\n",
    "                    max_cluster_data.append({'freq_idx':clust_freqs,'time_idx':clust_times})\n",
    "                else: \n",
    "                    continue\n",
    "\n",
    "        return max_cluster_data\n",
    "\n",
    "    def permuted_tfr_regression(self,alternative ='two-sided'):\n",
    "        \"\"\"\n",
    "        Run permuted tfr regression \n",
    "        \"\"\"\n",
    "\n",
    "        # permute predictor data\n",
    "        self.predictor_data = self.permute_predictor() # Permute predictor variable\n",
    "        # Run regression on permuted data + extract tstats only\n",
    "        perm_tstats = self.tfr_regression(permutation=True)  \n",
    "        # extract cluster statistics for permutation\n",
    "        perm_cluster_stat = self.max_tfr_cluster(perm_tstats, output='cluster_stat')  # Get cluster statistics\n",
    "        del perm_tstats  # Delete objects to free up memory\n",
    "        # check if any clusters detected in permuted tfr regression \n",
    "        if len(perm_cluster_stat) !=0 :\n",
    "            return perm_cluster_stat\n",
    "        else: # return list of empty cluster stats with length of alternative hypothesis ('two_sided'=length 2, less or greater = length 1)\n",
    "            return [{'cluster_stat':0}]*len(alternative.split('_'))\n",
    "\n",
    "    def pixel_regression(self,pixel_df):\n",
    "\n",
    "        \"\"\"\n",
    "        Run pixel-wise OLS regression model to extraxct beta coefficient and t-statistic. \n",
    "\n",
    "        Args:\n",
    "        - pixel_df (pandas df): regression dataframe (insert details here)\n",
    "\n",
    "        Returns:\n",
    "        - beta_coeff (numpy array): Beta coefficient(s) from pixel-wise regression.\n",
    "        - tstat_pixel (numpy array): Observed t-statistic(s) from pixel-wise regression.\n",
    "        \"\"\"\n",
    "\n",
    "        # formula should be in form 'col_name + col_name' if col is categorical then should be 'C(col_name)'  \n",
    "        formula    = 'pow ~ 1 + ' + (' + ').join(['C('+col+')' if pd.api.types.is_categorical_dtype(pixel_df[col])\n",
    "                                    else col for col in pixel_df.columns[~pixel_df.columns.isin(['pow'])].tolist()])\n",
    "\n",
    "        pixel_model = smf.ols(formula,pixel_df,missing='drop').fit() # fit regression model\n",
    "\n",
    "        return (pixel_model.params[self.permute_var],pixel_model.tvalues[self.permute_var])\n",
    "\n",
    "    def expand_tfr_indices(self):\n",
    "        iter_tup = list(map(tuple,np.unravel_index(np.dstack(([*np.indices(self.tfr_dims)])),np.product(self.tfr_dims))[0].\n",
    "                            reshape(np.product(np.dstack(([*np.indices(self.tfr_dims)])).shape[:2]),-1)))\n",
    "\n",
    "        return iter_tup\n",
    "\n",
    "    def make_pixel_df(self,epoch_data):\n",
    "        \"\"\"\n",
    "        Make pixel-level (frequency x timepoint) dataframe. \n",
    "        \"\"\"\n",
    "        return self.predictor_data.assign(pow=epoch_data)\n",
    "\n",
    "    def compute_tcritical(self,tails=2, alternative ='two-sided',alpha=0.05):\n",
    "        \"\"\"\n",
    "        Calculate critical t-values for regression model.\n",
    "\n",
    "        Args:\n",
    "        - predictor_dims (tuple): Dimensions of data matrix. Tuple of (n_samples, n_predictors). \n",
    "        - tails (int): Number of tails for t-distribution. Default is 2. Options are 1 or 2.\n",
    "        - alternative (str): Type of test. Default is 'two-sided'. Options are 'two-sided', 'greater', 'less'.\n",
    "        - alpha (float): Significance level. Default is 0.05.\n",
    "\n",
    "        Returns:\n",
    "        - tcritical (float): Critical t-value.\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate degrees of freedom\n",
    "        deg_free = float(len(self.predictor_data)-len(self.predictor_data.columns)-tails)\n",
    "\n",
    "        return (t.ppf(1-alpha/tails,deg_free) if alternative != 'less' else np.negative(t.ppf(1-alpha/tails,deg_free)))\n",
    "\n",
    "    def threshold_tfr_tstat(self,tfr_tstats,alternative='two-sided'):\n",
    "        if alternative == 'two-sided':\n",
    "            return [(tfr_tstats>self.compute_tcritical()).astype(int), (tfr_tstats<np.negative(self.compute_tcritical()).astype(int))]\n",
    "\n",
    "        elif alternative == 'greater':\n",
    "            return [(tfr_tstats>self.compute_tcritical(tails=1,alternative='greater')).astype(int)]\n",
    "\n",
    "        else: #alternative = less\n",
    "            return [(tfr_tstats<self.compute_tcritical(tails=1,alternative='less')).astype(int)]\n",
    "\n",
    "    def permute_predictor(self):\n",
    "        \"\"\"\n",
    "        Permute predictor variable for permutation test.\n",
    "        \"\"\"\n",
    "\n",
    "        self.predictor_data[self.permute_var] = np.random.permutation(self.predictor_data[self.permute_var].values)\n",
    "\n",
    "        return self.predictor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize list to store cluster data\n",
    "# cluster_list = []\n",
    "\n",
    "# for p in range(1000):\n",
    "#     uni_test = TFR_Cluster_Test(tfr_data,pd.DataFrame(test_univar),permute_var,1000)\n",
    "#     _, uni_tstats = uni_test.tfr_regression()\n",
    "#     cluster_data = uni_test.max_tfr_cluster(uni_tstats,output='cluster_stat') \n",
    "#     # add permutation number to cluster data\n",
    "#     cluster_data['perm_num'] = p\n",
    "#     del uni_test, uni_tstats # clear memory\n",
    "#     cluster_list.append(cluster_data) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TEST PERMUTATIONS \n",
    "# num_permutations = 1000\n",
    "# start = time.time() # start timer\n",
    "\n",
    "# all_ch_perm = {}\n",
    "\n",
    "# for c in range(num_channels):\n",
    "#         ch_start = time.time() # start timer\n",
    "\n",
    "#         # Prepare arguments for the permutation function\n",
    "#         permutation_args = [\n",
    "#         (np.squeeze(power_epochs._data[:,c,:,:]), reg_data, tcritical)\n",
    "#         for _ in range(num_permutations)]\n",
    "    \n",
    "#         # Perform permutations in parallel\n",
    "#         elec_permuted_data = Parallel(n_jobs=-1, verbose=12)(\n",
    "#         delayed(permuted_tfr_cluster_test)(*args)\n",
    "#         for args in permutation_args)\n",
    "        \n",
    "#         # save in all elec dict \n",
    "#         all_ch_perm[ch_names[c]] = elec_permuted_data\n",
    "#         pickle.dump(elec_permuted_data, open(f'{results_dir}{subj_id}_{ch_names[c]}_perm_clusters.pkl', \"wb\")) \n",
    "\n",
    "#         ch_end = time.time() \n",
    "#         print(f'{ch_names[c]} permute time: ', '{:.2f}'.format(ch_end-ch_start))\n",
    "        \n",
    "        \n",
    "\n",
    "# end = time.time()    \n",
    "# print('{:.2f} s'.format(end-start)) # print time elapsed for computation (approx 4 seconds per permutation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_permutations = 1000\n",
    "# ch_start = time.time() # start timer\n",
    "\n",
    "# # Prepare arguments for the permutation function\n",
    "# permutation_args = [\n",
    "# (np.squeeze(power_epochs._data[:,c,:,:]), reg_data, tcritical)\n",
    "# for _ in range(num_permutations)]\n",
    "\n",
    "# # Perform permutations in parallel\n",
    "# elec_permuted_data_reduc = Parallel(n_jobs=-1, verbose=12)(\n",
    "# delayed(permuted_tfr_cluster_test)(*args)\n",
    "# for args in permutation_args)\n",
    "\n",
    "# # save in all elec dict \n",
    "# # all_ch_perm[ch_names[c]] = elec_permuted_data\n",
    "# pickle.dump(elec_permuted_data_reduc, open(f'{results_dir}{subj_id}_{ch_names[c]}_reduced_output_perm_clusters.pkl', \"wb\")) \n",
    "\n",
    "# ch_end = time.time() \n",
    "# print(f'{ch_names[c]} permute time: ', '{:.2f}'.format(ch_end-ch_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To implement FDR correction: \n",
    "# https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html\n",
    "# multitest.multipletests(p_upper, method='fdr_bh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swb_ephys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
