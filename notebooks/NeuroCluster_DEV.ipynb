{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuroCluster:\n",
    "<font size= 4> Non-parametric cluster-based permutation testing to identify neurophysiological encoding of continuous variables with time-frequency resolution\n",
    "\n",
    "Authors: Christina Maher & Alexandra Fink-Skular \\\n",
    "Updated: 07/01/2024 by AFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from glob import glob\n",
    "from scipy.stats import zscore, t, linregress, ttest_ind, ttest_rel, ttest_1samp \n",
    "import os \n",
    "import re\n",
    "import h5io\n",
    "import pickle \n",
    "import time \n",
    "import datetime \n",
    "from joblib import Parallel, delayed\n",
    "import statsmodels.api as sm \n",
    "from scipy.ndimage import label \n",
    "import statsmodels.formula.api as smf\n",
    "import tqdm\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# keep this so we can use our respective paths for testing\n",
    "current_user = 'alie'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07012024\n"
     ]
    }
   ],
   "source": [
    "date = datetime.date.today().strftime('%m%d%Y')\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if current_user == 'christina':\n",
    "    base_dir = '/Users/christinamaher/Documents/GitHub/NeuroCluster/scripts/'\n",
    "    data_dir = '/Users/christinamaher/Documents/GitHub/NeuroCluster/'\n",
    "    tfr_dir  = f'{data_dir}tfr/'\n",
    "    anat_dir = f'{data_dir}anat/'\n",
    "elif current_user == 'alie':\n",
    "    # base_dir = '/hpc/users/finka03/NeuroCluster/NeuroCluster/'\n",
    "    # swb_dir  = '/sc/arion/projects/guLab/Alie/SWB/'\n",
    "    # tfr_dir  = f'{swb_dir}ephys_analysis/data/'\n",
    "    # beh_dir  = f'{swb_dir}swb_behav_models/data/behavior_preprocessed/'\n",
    "    # anat_dir = f'{swb_dir}ephys_analysis/recon_labels/'\n",
    "    # save_dir = f'{base_dir}/data/'\n",
    "    \n",
    "    base_dir = '/Users/alexandrafink/Documents/GraduateSchool/SaezLab/NeuroCluster/NeuroCluster/NeuroCluster/scripts/'\n",
    "    data_dir = '/Users/alexandrafink/Documents/GraduateSchool/SaezLab/SWB/'\n",
    "    tfr_dir  = f'{data_dir}ephys_analysis/data/'\n",
    "    beh_dir  = f'{data_dir}behavior_analysis/behavior_preprocessed/'\n",
    "    anat_dir = f'{data_dir}anat_recons/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load functions \n",
    "import sys\n",
    "sys.path.append(f'{base_dir}')\n",
    "# sys.path.append(f'{base_dir}scripts/')\n",
    "\n",
    "from tfr_cluster_test import *\n",
    "from helper_utils import *\n",
    "# from plotting_utils import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Format Input Data (Currently within-subject)\n",
    "- neural input: np.array (n_channels x n_epochs x n_freqs x n_times)\n",
    "- regressor data: np.array (numpy array: n_epochs x n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /Users/alexandrafink/Documents/GraduateSchool/SaezLab/SWB/ephys_analysis/data/MS002/MS002_CpeOnset-tfr.h5 ...\n",
      "Adding metadata with 19 columns\n"
     ]
    }
   ],
   "source": [
    "# load epoched data for single subj\n",
    "if current_user == 'alie':\n",
    "    permute_var = 'decisionCPE'\n",
    "    subj_id     = 'MS002'   \n",
    "    power_epochs = mne.time_frequency.read_tfrs(fname=f'{tfr_dir}{subj_id}/{subj_id}_CpeOnset-tfr.h5')[0]\n",
    "elif current_user == 'christina':\n",
    "    permute_var = 'ev_zscore'\n",
    "    subj_id     = 'MS009'   \n",
    "    power_epochs = mne.time_frequency.read_tfrs(fname=f'{tfr_dir}/{subj_id}_tfr.h5')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GambleChoice', 'TrialEV', 'TotalProfit', 'decisionCPE']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_reg_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set ROI for single ROI anaylsis \n",
    "if current_user == 'alie':\n",
    "#     roi = 'ains'\n",
    "    # set all variables included mutliple regression \n",
    "    multi_reg_vars = ['GambleChoice','TrialEV','TotalProfit','decisionCPE']\n",
    "    # set main variable of interest for permutations \n",
    "    permute_var = 'decisionCPE'\n",
    "    # load subj behavior data \n",
    "#     beh_df = pd.read_csv(f'{beh_dir}{subj_id}_task_data')\n",
    "    beh_df = power_epochs.metadata.copy()\n",
    "    # beh_df['subj_id'] = subj_id\n",
    "    # add TrialEV to df\n",
    "    beh_df['TrialEV'] = beh_df.GambleEV - beh_df.SafeBet\n",
    "    # clean subj dataframe from fail trials/nan values in vars of interest     \n",
    "    # beh_df = beh_df[(beh_df.GambleChoice=='gamble')|(beh_df.GambleChoice=='safe')]\n",
    "#     beh_df = beh_df[(beh_df.Outcome=='good')|(beh_df.Outcome=='bad')]\n",
    "    \n",
    "    # zscore continuous variables \n",
    "    beh_df[multi_reg_vars[1:]] = pd.DataFrame({f'{var}':zscore(beh_df[var])  for var in multi_reg_vars[1:]})\n",
    "    # format final beh_df\n",
    "    beh_df = beh_df[multi_reg_vars].reset_index(drop=True) \n",
    "    # convert choice to categorical variable\n",
    "    beh_df['GambleChoice'] = beh_df['GambleChoice'].astype('category')\n",
    "\n",
    "elif current_user == 'christina':\n",
    "    beh_df = prepare_regressor_df(power_epochs)\n",
    "    ## new function for getting elecs in ROI\n",
    "    roi = ['lpfc','ofc']\n",
    "    roi_subj_elecs = prepare_anat_dic(roi, f'{anat_dir}master_labels.csv')\n",
    "    roi_subj_elecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### class TFR_Cluster_Test dev + debugging\n",
    "\n",
    "if current_user == 'alie':\n",
    "\n",
    "    # subset single electrode tfr data + behav data\n",
    "    dev_ch_idx     = power_epochs.ch_names.index('laims2-laims3')\n",
    "    ch_name        = 'laims2-laims3'\n",
    "    tfr_data       = np.squeeze(power_epochs._data[:,dev_ch_idx,:,:].copy())\n",
    "    predictor_data = beh_df.copy()\n",
    "    \n",
    "    # predictor_data = predictor_data.drop(columns='subj_id')\n",
    "\n",
    "elif current_user == 'christina':\n",
    "    \n",
    "        # subset single electrode tfr data + behav data\n",
    "        # predictor_data = predictor_data.drop(columns=['condition','chosen_shape_current_trial','chosen_color_current_trial','chosen_shape_previous_trial','chosen_color_previous_trial','ev'])\n",
    "        tfr_data = np.squeeze(power_epochs._data[:,0,:,:].copy())\n",
    "        ch_name = power_epochs.info['ch_names'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Find Real Clusters\n",
    "- Use TFRClusterTest class code to run multivariate regression\n",
    "- Allows for multiple regression implementation and pixel paralellization, so with more speed improvements will ultimately be worth it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TFR_Cluster_Test at 0x7f99b8417c10>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_test  = TFR_Cluster_Test(tfr_data,predictor_data,permute_var,ch_name,alternative='two-sided')\n",
    "cluster_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:    4.3s\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:    4.6s\n",
      "[Parallel(n_jobs=-1)]: Done 952 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2968 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 5560 tasks      | elapsed:   11.8s\n",
      "[Parallel(n_jobs=-1)]: Done 8728 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=-1)]: Done 12472 tasks      | elapsed:   22.3s\n",
      "[Parallel(n_jobs=-1)]: Done 16792 tasks      | elapsed:   28.2s\n",
      "[Parallel(n_jobs=-1)]: Done 21688 tasks      | elapsed:   33.7s\n",
      "[Parallel(n_jobs=-1)]: Done 27160 tasks      | elapsed:   39.5s\n",
      "[Parallel(n_jobs=-1)]: Done 33208 tasks      | elapsed:   46.2s\n",
      "[Parallel(n_jobs=-1)]: Done 39832 tasks      | elapsed:   53.5s\n",
      "[Parallel(n_jobs=-1)]: Done 45030 out of 45030 | elapsed:   59.7s finished\n"
     ]
    }
   ],
   "source": [
    "betas, tstats = cluster_test.tfr_regression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cluster_data  = cluster_test.max_tfr_cluster(tstats,output='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cluster_stat': 813.3582360946806,\n",
       "  'freq_idx': (11, 19),\n",
       "  'time_idx': (276, 395)},\n",
       " {'cluster_stat': -160.23388387421727,\n",
       "  'freq_idx': (15, 16),\n",
       "  'time_idx': (437, 470)}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_cluster_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BELOW NOT FULLY IMPLEMENTED - FOR XTINA TO FINISH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NeuroCluster single electrode workflow: \n",
    "\n",
    "# Step 1: Create TFR_Cluster_Test Object\n",
    "cluster_test  = TFR_Cluster_Test(tfr_data,predictor_data,permute_var,ch_name,alternative='two-sided')\n",
    "\n",
    "# Step 2: Run TFR regression to extract beta coefficients for predictor of interest (permute_var) & tstats for each pixel in TFR\n",
    "betas, tstats = cluster_test.tfr_regression()\n",
    "\n",
    "# Step 3: Find largest cluster(s) and return the max cluster statistic(s) and cluster's  frequencies x times indices\n",
    "max_cluster_data  = cluster_test.max_tfr_cluster(tstats,output='all')\n",
    "\n",
    "# Step 4: Create null distribution of maximum cluster statistics from permuted data\n",
    "null_cluster_distribution = cluster_test.compute_null_cluster_stats(max_cluster_data,num_permutations=10)\n",
    "\n",
    "# Step 5: Use null cluster statistic distribution from permutations to compute non-parametric p value \n",
    "cluster_pvalue = cluster_test.cluster_significance_test(max_cluster_data,null_cluster_distribution,alpha=0.05) #compute_cluster_pvalue cluster_significance_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFR_Cluster_Test(object):\n",
    "    \"\"\" \n",
    "    Single-electrode neurophysiology object class to identify time-frequency resolved neural activity correlates of complex behavioral variables using non-parametric \n",
    "    cluster-based permutation testing.   \n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    tfr_data       : (np.array) Single electrode tfr data matrix. Array of floats (n_epochs,n_freqs,n_times). \n",
    "    tfr_dims       : (tuple) Frequency and time dimensions of tfr_data. Tuple of integers (n_freq,n_times). \n",
    "    ch_name        : (str) Unique electrode identification label. String of characters.\n",
    "    predictor_data : (pd.DataFrame) Regressors from task behavior with continuous, discreet, or categorical data. DataFrame of (rows=n_epochs,columns=n_regressors). \n",
    "    permute_var    : (str) Column label for primary regressor of interest.\n",
    "      \n",
    "    Methods\n",
    "    ----------\n",
    "    **To-do: fill in methods info\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tfr_data, predictor_data, permute_var, ch_name, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - tfr_data       : (np.array) Single electrode tfr data matrix. Array of floats (n_epochs,n_freqs,n_times). \n",
    "        - predictor_data : (pd.DataFrame) Task-based regressor data with dtypes continuous/discreet(int64/float) or categorical(pd.Categorical). DataFrame of (n_epochs,n_regressors).\n",
    "        - permute_var    : (str) Column label for primary regressor of interest. Array of 1d integers or floats (n_epochs,).\n",
    "        - ch_name        : (str) Unique electrode identification label. String of characters.  \n",
    "        - **kwargs       : (optional) alternative, alpha, cluster_shape\n",
    "        \"\"\"\n",
    "\n",
    "        self.tfr_data       = tfr_data  # single electrode tfr data\n",
    "        self.predictor_data = predictor_data # single subject behav data\n",
    "        self.tfr_dims       = self.tfr_data.shape[1:] # time-frequency dims of electrode data (n_freqs x n_times)\n",
    "        self.permute_var    = permute_var # variable to permute in regression model \n",
    "        self.ch_name        = ch_name # channel name for single electrode tfr data\n",
    "\n",
    "    def tfr_regression(self):\n",
    "        \"\"\"\n",
    "        Performs univariate or multivariate OLS regression across tfr matrix for all pixel-level time-frequency power data and task-based predictor variables. Regressions are parallelized across pixels.\n",
    "\n",
    "        Returns:\n",
    "        - tfr_betas  : (np.array) Matrix of beta coefficients for predictor of interest for each pixel regression. Array of (n_freqs,n_times). \n",
    "        - tfr_tstats : (np.array) Matrix of t-statistics from coefficient estimates for predictor of interest for each pixel regression. Array of (n_freqs,n_times). \n",
    "        \"\"\"\n",
    "        \n",
    "        # Prepare arguments for parallelization`using tfr matrix indices converted to list of tuples (freq x power)\n",
    "        pixel_args = [self.make_pixel_df(self.tfr_data[:,freq_idx,time_idx]) for freq_idx,time_idx in self.expand_tfr_indices()]\n",
    "        \n",
    "        # run pixel permutations in parallel \n",
    "        expanded_results = Parallel(n_jobs=-1, verbose=5)(\n",
    "                        delayed(self.pixel_regression)(args)\n",
    "                            for args in pixel_args)      \n",
    "\n",
    "        # preallocate np arrays for betas + tstats\n",
    "        tfr_betas  = np.zeros((self.tfr_dims))\n",
    "        tfr_tstats = np.zeros((self.tfr_dims))\n",
    "\n",
    "        # expanded_results is a list of tuples (beta,tstat) for every pixel \n",
    "        for count,(freq_idx,time_idx) in enumerate(self.expand_tfr_indices()):\n",
    "            tfr_betas[freq_idx,time_idx]  = expanded_results[count][0]\n",
    "            tfr_tstats[freq_idx,time_idx] = expanded_results[count][1]\n",
    "        \n",
    "        return tfr_betas, tfr_tstats\n",
    "\n",
    "    def pixel_regression(self,pixel_df):\n",
    "        \"\"\"\n",
    "        Fit pixel-wise univariate or multivariate OLS regression model and extract beta coefficient and t-statistic for predictor of interest (self.permute_var). \n",
    "\n",
    "        Args:\n",
    "        - pixel_df   : (pd.DataFrame) Pixel-level regression dataframe with power epochs data and behavioral regressors. DataFrame of (n_epochs, n_regressors+1). \n",
    "                                      Regressor column data must be continuous(dtype=float), discrete(dtype=int), or categorical(dtype=pd.Categorical). \n",
    "        \n",
    "        Returns:\n",
    "        - pixel_beta : (np.array) Beta coefficient for predictor of interest from pixel-wise regression. Array of 1d float (1,)\n",
    "        - pixel_tval : (np.array) Observed t-statistic for predictor of interest from pixel-wise regression. Array of 1d float (1,)\n",
    "        \"\"\"\n",
    "\n",
    "        # formula should be in form 'col_name + col_name' if col is categorical then should be 'C(col_name)'  \n",
    "        formula    = '+ '.join(['pow ~ 1 ',(' + ').join([''.join(['C(',col,')']) if pd.api.types.is_categorical_dtype(pixel_df[col])\n",
    "                            else col for col in pixel_df.columns[~pixel_df.columns.isin(['pow'])].tolist()])])\n",
    "        \n",
    "        pixel_model = smf.ols(formula,pixel_df,missing='drop').fit()\n",
    "\n",
    "        return (pixel_model.params[self.permute_var],pixel_model.tvalues[self.permute_var])\n",
    "\n",
    "    def max_tfr_cluster(self,tfr_tstats,alternative='two-sided',output='all',clust_struct=np.ones(shape=(3,3))):\n",
    "\n",
    "        \"\"\"\n",
    "        Identify time-frequency clusters of neural activity that are significantly correlated with the predictor of interest (self.permute_var). Clusters are identified \n",
    "        from neighboring pixel regression t-statistics for the predictor of interest that exceed the tcritical threshold from the alternate hypothesis. \n",
    "\n",
    "        Args:\n",
    "        - tfr_tstats       : (np.array) Pixel regression tstatistic from coefficient estimates for predictor of interest. Array of floats (n_freqs,n_times). \n",
    "        - alternative      : (str) Alternate hypothesis for t-test. Must be 'two-sided','greater', or 'less'. Default is 'two-sided'. \n",
    "        - output           : (str) Output format for max cluster statistics. Must be 'all', 'cluster_stat', or 'freq_time'. Default is 'all'.\n",
    "        - clust_struct     : (np.array) Binary matrix to specify cluster structure for scipy.ndimage.label. Array of (3,3). \n",
    "                                        Default is np.ones.shape(3,3), to allow diagonal cluster pixels (Not the scipy.ndimage.label default).\n",
    "                                        https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.label.html\n",
    "\n",
    "        Returns:\n",
    "        - max_cluster_data : (list) Beta coefficient for predictor of interest for each pixel regression. List (len=2 if 'two-sided') of dict(s).\n",
    "                                    If output = 'all', return dictionary of maximum cluster statistic ('cluster_stat' : sum of pixel t-statistics), \n",
    "                                    cluster frequency indices ('freq_idx':(freq_x,freq_y)), and cluster time indices ('time_idx':(time_x,time_y)). \n",
    "                                    If output = 'cluster_stat', return only [{cluster_stat}]. If output = 'freq_time', return only {freq_idx,time_idx}\n",
    "                                    ** If no clusters are found, max_cluster_data = {[]}\n",
    "        *** add docstring for expanded output\n",
    "        \"\"\"\n",
    "        \n",
    "        max_cluster_data = []\n",
    "        # Create binary matrix from tfr_tstats by thresholding pixel t-statistics by tcritical. (1 = pixel t-statistic exceeded tcritical threshold)\n",
    "        for binary_mat in self.threshold_tfr_tstat(tfr_tstats,alternative):\n",
    "\n",
    "            # test whether there are any pixels above tcritical threshold\n",
    "            if np.sum(binary_mat) != 0: \n",
    "                # Find clusters of pixels with t-statistics exceeding tcritical\n",
    "                cluster_label, num_clusters = label(binary_mat,clust_struct)\n",
    "                # use argmax to find index of largest absolute value of cluster t statistic sums \n",
    "                max_label = np.argmax([np.abs(np.sum(tfr_tstats[cluster_label==i+1])) for i in range(num_clusters)])+1\n",
    "                # use max_label index to compute cluster tstat sum (without absolute value)\n",
    "                max_clust_stat = np.sum(tfr_tstats[cluster_label==max_label])\n",
    "                # find 2D indices of minimum/maximum cluster frequencies and times \n",
    "                clust_freqs, clust_times = [(np.min(arr),np.max(arr)) for arr in np.where(cluster_label == max_label)]\n",
    "\n",
    "                if output == 'all':\n",
    "                    max_cluster_data.append({'cluster_stat':max_clust_stat,'freq_idx':clust_freqs,'time_idx':clust_times})\n",
    "                elif output == 'cluster_stat':\n",
    "                    max_cluster_data.append({'cluster_stat':max_clust_stat})\n",
    "                elif output == 'freq_time':\n",
    "                    max_cluster_data.append({'freq_idx':clust_freqs,'time_idx':clust_times})\n",
    "                elif output == 'expanded':\n",
    "                    max_cluster_data.append({'cluster_stat':max_clust_stat,'freq_idx':clust_freqs,'time_idx':clust_times,\n",
    "                                            'max_label':max_label,'all_clusters':cluster_label})\n",
    "            \n",
    "            else: # if there is no cluster, keep max_cluster_data empty list\n",
    "                continue\n",
    "            \n",
    "        return max_cluster_data\n",
    "\n",
    "    def compute_tcritical(self,alternative ='two-sided',alpha=0.05):\n",
    "        \"\"\"\n",
    "        Calculate critical t-values for regression model.\n",
    "        \n",
    "        Args:\n",
    "        - alternative : (str) Alternate hypothesis for t-test. Must be 'two-sided','greater', or 'less'. Default is 'two-sided'.\n",
    "        - alpha       : (float) Significance level. Default is 0.05.\n",
    "\n",
    "        Returns:\n",
    "        - tcritical   : (float) Critical t-statistic for hypothesis test. Positive value when alternative = 'two-sided' or 'greater'. Negative when alternative = 'less'. \n",
    "        \"\"\"\n",
    "\n",
    "        # Set number of tails for t-tests using 'alternative' parameter input string. \n",
    "            # tails = 2 if alternative = 'two-sided' (two tailed hypothesis test)\n",
    "            # tails = 1 if alternative = 'greater' or 'less' (one tailed hypothesis test)\n",
    "        tails = len(alternative.split('-')) \n",
    "\n",
    "        # Calculate degrees of freedom (N-k-1) \n",
    "        deg_free = float(len(self.predictor_data)-len(self.predictor_data.columns)-1) #### predictor data must only include regressors in columns\n",
    "\n",
    "        # Return tcritical from t-distribution. Significance level is alpha/2 for two tailed hypothesis tests (alternative = 'two-sided').\n",
    "        return (t.ppf(1-(alpha/tails),deg_free) if alternative != 'less' else np.negative(t.ppf(1-(alpha/tails),deg_free)))\n",
    "\n",
    "    def threshold_tfr_tstat(self,tfr_tstats,alternative='two-sided'):\n",
    "        \"\"\"\n",
    "        Threshold tfr t-statistic matrix using tcritical.\n",
    "\n",
    "        Args:\n",
    "        - tfr_tstats  : (np.array) Matrix of t-statistics from pixel-wise regressions. Array of floats (n_freqs, n_times). \n",
    "        - alternative : (str) Type of hypothesis test for t-distribution. Must be 'two-sided', 'greater', 'less'. Default is 'two-sided'.\n",
    "\n",
    "        Returns:\n",
    "        - binary_mat  : (np.array) Binary matrix results of pixel-wise t-tests. Pixel = 1 when tstatistic > tcritical, else pixel = 0. List of array(s) (n_freqs, n_times).\n",
    "        \"\"\"\n",
    "\n",
    "        if alternative == 'two-sided': \n",
    "            return [(tfr_tstats>self.compute_tcritical()).astype(int), (tfr_tstats<np.negative(self.compute_tcritical())).astype(int)]\n",
    "\n",
    "        elif alternative == 'greater':\n",
    "            return [(tfr_tstats>self.compute_tcritical(tails=1,alternative='greater')).astype(int)]\n",
    "\n",
    "        elif alternative == 'less':\n",
    "            return [(tfr_tstats<self.compute_tcritical(tails=1,alternative='less')).astype(int)] \n",
    "        else: \n",
    "            raise ValueError('Alternative hypothesis must be two-sided, greater, or less not {alternative}')\n",
    "    \n",
    "    def expand_tfr_indices(self):\n",
    "        \"\"\"\n",
    "        Create list of tfr pixel indices for parallelized tfr_regression.\n",
    "\n",
    "        Returns:\n",
    "        - iter_tup : (list) Time-frequency indices for all pixels in tfr_data. List of tuples [(freq_x_index,freq_y_index),(time_x_index,time_y_index)]        \n",
    "        \"\"\"\n",
    "\n",
    "        return list(map(tuple,np.unravel_index(np.dstack(([*np.indices(self.tfr_dims)])),np.product(self.tfr_dims)\n",
    "                            )[0].reshape(np.product(np.dstack(([*np.indices(self.tfr_dims)])).shape[:2]),-1)))\n",
    "\n",
    "    def make_pixel_df(self,epoch_data,permuted=False):\n",
    "        \"\"\"\n",
    "        Format input data for pixel regression.  input data. Make pixel-level (frequency x timepoint) dataframe. Add tfr power data for single pixel to predictor_df. \n",
    "\n",
    "        Args:\n",
    "        - epoch_data : (str) Alternate hypothesis for t-test. Must be 'two-sided','greater', or'less'. Default is 'two-sided'. Array of 1d integers or floats (n_epochs,).\n",
    "        \n",
    "        Returns:\n",
    "        - pixel_df   : (pd.DataFrame) Pixel regression input dataframe containing power epochs and task-based behavioral regressor data (dtype=int/float/pd.Categorical). \n",
    "                                      DataFrame of (n_epochs, n_regressors+1). \n",
    "        \n",
    "        ##### to-do add docstring info for permuted kwargs\n",
    "        \"\"\"\n",
    "        if permuted: ###### make clear that this permanently updates predictor data!!!!\n",
    "            self.predictor_data[self.permute_var] = np.random.permutation(self.predictor_data[self.permute_var].values)\n",
    "            return self.predictor_data.assign(pow=epoch_data)\n",
    "        else: \n",
    "            return self.predictor_data.assign(pow=epoch_data) \n",
    "\n",
    "###### UNTESTED PERMUTATION FUNCTIONS!!!\n",
    "\n",
    "    def compute_null_cluster_stats(self,num_permutations=None):\n",
    "\n",
    "        #### for every permutation:\n",
    "            # permute predictor of interest, then make pixel df \n",
    "            # run tfr regression & extract permutation t stats \n",
    "            # find max cluster statistics for permutation  \n",
    "\n",
    "        null_cluster_distribution = Parallel(n_jobs=-1, verbose=5)(delayed\n",
    "                                            (self.max_tfr_cluster(output='cluster_stat'))(self.permuted_tfr_regression) for n in num_permutations)\n",
    "        return null_cluster_distribution\n",
    "\n",
    "\n",
    "    def permuted_tfr_regression(self):\n",
    "        \"\"\"\n",
    "        Run tfr regression for single permutation\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        iter_tup = self.expand_tfr_indices()\n",
    "\n",
    "        # either precompute pixel_args before passing to parallel, or run all together in loop. - check later!! \n",
    "        perm_args = [self.make_pixel_df(self.tfr_data[:,freq_idx,time_idx],permuted=True) for freq_idx,time_idx in iter_tup]\n",
    "\n",
    "        # Run regression on permuted data + extract tstats only\n",
    "\n",
    "        # run pixel permutations in parallel \n",
    "        permuted_results = Parallel(n_jobs=-1, verbose=5)(\n",
    "                        delayed(self.pixel_regression)(args)\n",
    "                            for args in perm_args)      \n",
    "        \n",
    "        # preallocate np arrays for betas + tstats\n",
    "        perm_tstats = np.zeros((self.tfr_dims))\n",
    "\n",
    "        # expanded_results is a list of tuples (beta,tstat) for every pixel \n",
    "        for count,(freq_idx,time_idx) in enumerate(iter_tup):\n",
    "            perm_tstats[freq_idx,time_idx] = permuted_results[count][1]\n",
    "        \n",
    "        return perm_tstats\n",
    "    \n",
    "\n",
    "\n",
    "    # def cluster_significance_test(self, null_distribution,max_cluster_stat,alpha=0.05,alternative='two-sided'):\n",
    "    #     \"\"\"\n",
    "    #     Compute non-param etric pvalue from cluster permutation data \n",
    "\n",
    "    #             - alpha (float): Significance level. Default is 0.05.\n",
    "\n",
    "        # null_df = pd.concat([pd.DataFrame(dict,index=[0]) for dict in null_distribution]).reset_index(drop=True)\n",
    "        # null_df['sign'] = ['positive' if row.cluster_stat > 0 else 'negative' for row in null_df.iterrows()]\n",
    "        # for sign in null_df.sign.unique(): #### one loop option \n",
    "        # for cluster in max_cluster_stat: ### another loop option\n",
    "        #     null_max_clusters = null_df.cluster_stat[null_df.sign == sign]\n",
    "\n",
    "\n",
    "    #     \"\"\"\n",
    "        \n",
    "    #     return cluster_pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time() # start timer\n",
    "\n",
    "# ## run simple linear regression on electrodes in parallel to speed up computation - I did this for just the subset OFC electrodes.\n",
    "# subj_all_elec_data = Parallel(n_jobs=-1,verbose=5)(\n",
    "#     delayed(TFR_Cluster_Test.tfr_cluster_results())(\n",
    "#         np.squeeze(power_epochs._data[:,ch_ix,:,:].copy()),predictor_data,permute_var,ch_name\n",
    "#         ) for ch_ix, ch_name in enumerate(power_epochs.ch_names))\n",
    "\n",
    "# end = time.time()    \n",
    "# print('{:.4f} s'.format(end-start)) # print time elapsed for computation (approx 20 seconds per channel)\n",
    "\n",
    "# # save subj cluster data for all electrodes\n",
    "# pickle.dump(subj_all_elec_data, open(f'{save_dir}{subj_id}_all_elec_real_clusters.pkl', \"wb\")) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To implement FDR correction: \n",
    "# https://www.statsmodels.org/dev/generated/statsmodels.stats.multitest.multipletests.html\n",
    "# multitest.multipletests(p_upper, method='fdr_bh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swb_ephys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
